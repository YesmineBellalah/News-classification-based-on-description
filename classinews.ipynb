{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "f= csv.writer(open(\"/Users/shangavi/Desktop/AllNews.csv\",\"w+\"))\n",
    "\n",
    "#Article tirer d'une API\n",
    "url = ('https://newsapi.org/v2/everything?q=politics&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "data = response.json()['articles']\n",
    "data\n",
    "\n",
    "# Création des colonnes \n",
    "f.writerow([\"Description\", \"Catégorie\"])\n",
    "for i in data :\n",
    "      f.writerow([i['description'],'POLITICS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = ('https://newsapi.org/v2/everything?q=arts&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=wellness&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=entertainment&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=travel&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=style & beauty&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=parenting&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=food & drink&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=queer voices&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=healthy living&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=business&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=comedy&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=sports&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=black voices&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=home & living&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "url = ('https://newsapi.org/v2/everything?q=the worldpost&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=weedings&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=parents&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=divorce&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=impact&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=women&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=crime&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=media&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=world news&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=weird news&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=green&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=tech&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=taste&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=religion&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=science&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=money&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=style&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=arts & culture&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=environment&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=worldpost&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=fifty&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=good news&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=latino voices&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=culture & arts&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=college&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "#url = ('https://newsapi.org/v2/everything?q=education&apiKey=c0b5c7d11ecc4d80880d3845e85efc59')\n",
    "response = requests.get(url)\n",
    "data = response.json()['articles']\n",
    "for i in data :\n",
    "      f.writerow([i['description'],'THE WORLDPOST'])#'ARTS' #'WELLNESS'#'ENTERTAINMENT' #'TRAVEL' #'STYLE & BEAUTY' #'PARENTING' #'FOOD & DRINK'\n",
    "                 #'QUEER VOICES' #'HEALTHY LIVING' #'BUSINESS' #'COMEDY' #'SPORTS' #'BLACK VOICES' #'HOME & LIVING' #'THE WORLDPOST' #'WEDDINGS' #'PARENTS'\n",
    "                 #'DIVORCE' #'IMPACT' #'WOMEN' #'CRIME' #'MEDIA' #'WORLD NEWS' #'WEIRD NEWS' #'GREEN' #'TECH' #'TASTE' #'RELIGION' #'SCIENCE' #'MONEY'\n",
    "                 #'STYLE' #'ARTS & CULTURE' #'ENVIRONMENT' #'WORLDPOST' #'FIFTY' #'GOOD NEWS' #'LATINO VOICES' #'CULTURE & ARTS' #'COLLEGE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Packages needed\n",
    "from __future__ import print_function\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english')) \n",
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np \n",
    "from string import punctuation\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe pour faire le nettoyage du texte: suppression des caracteres speciaux, des stopwords, lemmatisation et tokenization\n",
    "#preprocessing texte\n",
    "with open('stopwors.txt') as swfile:\n",
    "                stopwords = set(swfile.read().split('\\n'))\n",
    "        \n",
    "class TextCleaner(object):\n",
    "\n",
    "    def __init__(self, stopwords_file=None,\n",
    "                 substitutions_file=None):\n",
    "        \"\"\"\n",
    "        Constructor of the TextCleaner instance.\n",
    "        Args:\n",
    "            stopwords_file (optional): path to .txt file containing stopwords (one by row).\n",
    "            'None' if no stopword removal is needed.\n",
    "            substitutions_file:  path to .json file containing words to substitute (example: \"aren't\" to \"are not\").\n",
    "            The .json file is structured as a list of two element lists: [[old_pattern, new_pattern], ...]\n",
    "        \"\"\"\n",
    "\n",
    "        # Compile useful regexes\n",
    "        self.re_url = re.compile(r\"https{0,1}:\\/\\/[^\\s]*\")\n",
    "        self.re_mail = re.compile(r\"[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}\")\n",
    "        self.re_special = re.compile(r\"[\\(\\)\\*\\[\\]\\/,\\-=+#{}'<>:;\\\"\\n]+\")\n",
    "        self.re_sentencize = re.compile(r\"((?<!mr|ms)[.?!]+)\")\n",
    "        self.re_tokenize = re.compile(r\"[\\s\\.\\!\\?]+\")\n",
    "        self.re_emoji_pattern= re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "                \"]+\", flags=re.UNICODE)\n",
    "        #self.re_mentions=re.compile(r\"\\s([@#][\\w_-]+)\")\n",
    "\n",
    "    \n",
    "        # read stopwords\n",
    "        if stopwords_file is not None:\n",
    "            with open(stopwords_file) as swfile:\n",
    "                self.stopwords = set(swfile.read().split('\\n'))\n",
    "        else:\n",
    "            self.stopwords = set()\n",
    "\n",
    "        # read substitutions\n",
    "        if substitutions_file is not None:\n",
    "            with open(substitutions_file) as subfile:\n",
    "                self.substitutions = json.load(subfile)\n",
    "        else:\n",
    "            self.substitutions = {}\n",
    "            \n",
    "    def remove_mentions(self, document):\n",
    "        document=\" \".join(filter(lambda x:x[0]!='@', document.split()))\n",
    "        return document.strip()\n",
    "    \n",
    "    def add_stopwords(self, stopwords):\n",
    "        \"\"\"\n",
    "        Complete the stopwords list.\n",
    "        Args:\n",
    "            stopwords: iterable containing stopwords as strings\n",
    "\n",
    "        Returns: None\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.stopwords |= set(stopwords)\n",
    "\n",
    "    def add_substitutions(self, substitutions):\n",
    "        \"\"\"\n",
    "        Complete the substitutions list.\n",
    "        Args:\n",
    "            stopwords: iterable containing lists of 2 elements ([old_pattern, new_pattern])\n",
    "\n",
    "        Returns: None\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.substitutions += list(substitutions)\n",
    "\n",
    "    def remove_urls(self, document, replace=r' '):\n",
    "        \"\"\"\n",
    "        Remove urls from a document.\n",
    "        Args:\n",
    "            document: string\n",
    "            replace (optional): pattern replacing urls (default: whitespace)\n",
    "\n",
    "        Returns: pruned document (string)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        document = self.re_url.sub(replace, document)\n",
    "        return document.strip()\n",
    "\n",
    "    def remove_mails(self, document, replace=r' '):\n",
    "        \"\"\"\n",
    "        Remove e-mail addresses from a document.\n",
    "        Args:\n",
    "            document: string\n",
    "            replace (optional): pattern replacing mail addresses (default: whitespace)\n",
    "\n",
    "        Returns: pruned document (string)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        document = self.re_mail.sub(replace, document)\n",
    "        return document.strip()\n",
    "\n",
    "    def remove_special_characters(self, document, replace=r' '):\n",
    "        \"\"\"\n",
    "        Remove special characters from a document (punctuation, except from end-of-sentences).\n",
    "        Args:\n",
    "            document: string\n",
    "            replace (optional, default whitespace): pattern replacing special characters\n",
    "\n",
    "        Returns: pruned document (string)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        document = self.re_special.sub(replace, document)\n",
    "        return document.strip()\n",
    "    def remove_emoji(self, document, replace=r''):\n",
    "        \"\"\"\n",
    "        Remove emoticons, symbols, flags from a document\n",
    "        Args:\n",
    "            document: string\n",
    "            replace (optional, default whitespace): pattern replacing special characters\n",
    "        Returns: purned document (string)\n",
    "        \"\"\"\n",
    "\n",
    "        document=self.re_emoji_pattern.sub(r'', document)\n",
    "        return(document)\n",
    "    def clean(self, document):\n",
    "        \"\"\"\n",
    "        Lower the document, remove urls, mails, special characters and emojis.\n",
    "        Args:\n",
    "            document: string\n",
    "\n",
    "        Returns: pruned document (string)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        document = document.lower().strip()\n",
    "        #document=re.sub(r'&amp', ' ',document)\n",
    "        document = self.remove_urls(document)\n",
    "        document = self.remove_mails(document)\n",
    "        document = self.remove_special_characters(document)\n",
    "        document = self.remove_emoji(document)\n",
    "        document =self.remove_mentions(document)\n",
    "        return document\n",
    "\n",
    "    def sentencize(self, document):\n",
    "        \"\"\"\n",
    "        Cut a document into sentences.\n",
    "        Args:\n",
    "            document: string\n",
    "\n",
    "        Returns: list of strings\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        sentences = self.re_sentencize.split(document)\n",
    "        sentences = [s.strip() for i, s in enumerate(sentences) if not i % 2]\n",
    "        if sentences[-1] == '':\n",
    "            sentences = sentences[:-1]\n",
    "        return sentences\n",
    "\n",
    "    def tokenize(self, document, remove_stopwords=False, substitute=False,\n",
    "                 lemmatizer=None):\n",
    "        \"\"\"\n",
    "        Cut a document into tokens.\n",
    "        Args:\n",
    "            document: string\n",
    "            remove_stopwords (optional, default False): Whether or not stopwords must be removed\n",
    "            substitute (optional, default False): Whether or not substitutions must be applied\n",
    "            lemmatizer (Lemmatizer object, default None): if not None, a lemmatization operation is performed. The\n",
    "            lemmatizer must have a method called \"lemmatize\" that take as argument a word and returns its lemma.\n",
    "\n",
    "        Returns: tokenized words\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if substitute:\n",
    "            for old, new in self.substitutions:\n",
    "                document = re.sub(old, new, document)\n",
    "\n",
    "        tokens = self.re_tokenize.split(document)\n",
    "\n",
    "        if remove_stopwords:\n",
    "           \n",
    "            tokens = [t for t in tokens if t not in self.stopwords  and t.isalpha() and len(t)>1]\n",
    "\n",
    "        if lemmatizer:\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200853, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category        date  \\\n",
       "0  Melissa Jeltsen          CRIME  2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT  2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT  2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT  2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT  2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \n",
       "0  She left her husband. He killed their children...  \n",
       "1                           Of course it has a song.  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  \n",
       "4  The \"Dietland\" actress said using the bags is ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "with open('News_Category_Dataset_v2.json') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "df=pd.DataFrame(data)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178353, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.drop_duplicates('short_description')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x29647066748>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of length of short description \n",
    "df.short_description.map(len).hist(figsize=(15, 5), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_description</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>CRIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   short_description       category\n",
       "0  She left her husband. He killed their children...          CRIME\n",
       "1                           Of course it has a song.  ENTERTAINMENT\n",
       "2  The actor and his longtime girlfriend Anna Ebe...  ENTERTAINMENT\n",
       "3  The actor gives Dems an ass-kicking for not fi...  ENTERTAINMENT\n",
       "4  The \"Dietland\" actress said using the bags is ...  ENTERTAINMENT"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=df[['short_description','category']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_description</th>\n",
       "      <th>category</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>she left her husband. he killed their children...</td>\n",
       "      <td>[left, husband, killed, child, day, america]</td>\n",
       "      <td>left husband killed child day america</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>of course it has a song.</td>\n",
       "      <td>[course, song]</td>\n",
       "      <td>course song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>the actor and his longtime girlfriend anna ebe...</td>\n",
       "      <td>[actor, longtime, girlfriend, anna, eberstein,...</td>\n",
       "      <td>actor longtime girlfriend anna eberstein tied ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>the actor gives dems an ass kicking for not fi...</td>\n",
       "      <td>[actor, dems, as, kicking, fighting, hard, don...</td>\n",
       "      <td>actor dems as kicking fighting hard donald trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>the dietland actress said using the bags is a ...</td>\n",
       "      <td>[dietland, actress, bag, cathartic, therapeuti...</td>\n",
       "      <td>dietland actress bag cathartic therapeutic moment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   short_description       category  \\\n",
       "0  She left her husband. He killed their children...          CRIME   \n",
       "1                           Of course it has a song.  ENTERTAINMENT   \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  ENTERTAINMENT   \n",
       "3  The actor gives Dems an ass-kicking for not fi...  ENTERTAINMENT   \n",
       "4  The \"Dietland\" actress said using the bags is ...  ENTERTAINMENT   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  she left her husband. he killed their children...   \n",
       "1                           of course it has a song.   \n",
       "2  the actor and his longtime girlfriend anna ebe...   \n",
       "3  the actor gives dems an ass kicking for not fi...   \n",
       "4  the dietland actress said using the bags is a ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0       [left, husband, killed, child, day, america]   \n",
       "1                                     [course, song]   \n",
       "2  [actor, longtime, girlfriend, anna, eberstein,...   \n",
       "3  [actor, dems, as, kicking, fighting, hard, don...   \n",
       "4  [dietland, actress, bag, cathartic, therapeuti...   \n",
       "\n",
       "                                                text  \n",
       "0              left husband killed child day america  \n",
       "1                                        course song  \n",
       "2  actor longtime girlfriend anna eberstein tied ...  \n",
       "3   actor dems as kicking fighting hard donald trump  \n",
       "4  dietland actress bag cathartic therapeutic moment  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl=TextCleaner(None, None)\n",
    "cl.add_stopwords(stopwords)\n",
    "data[\"clean_text\"]=[cl.clean(w) for w in data[\"short_description\"]]\n",
    "data[\"tokens\"]=[cl.tokenize(w, remove_stopwords=True, substitute=False,lemmatizer=lemmatizer) for w in data[\"clean_text\"]]\n",
    "data['text']=[\" \".join (token) for token in data['tokens']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max length\n",
    "data['len']=[len(token) for token in data.tokens]\n",
    "max(data['len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63605"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max nb words\n",
    "tokens=[]\n",
    "for i in data.index:\n",
    "    for t in data.loc[i,'tokens']:\n",
    "            tokens.append(t)\n",
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>POLITICS</th>\n",
       "      <td>29131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WELLNESS</th>\n",
       "      <td>17550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTERTAINMENT</th>\n",
       "      <td>13271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRAVEL</th>\n",
       "      <td>9377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STYLE &amp; BEAUTY</th>\n",
       "      <td>9133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PARENTING</th>\n",
       "      <td>8621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOOD &amp; DRINK</th>\n",
       "      <td>6189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QUEER VOICES</th>\n",
       "      <td>5399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HEALTHY LIVING</th>\n",
       "      <td>5196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BUSINESS</th>\n",
       "      <td>5056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMEDY</th>\n",
       "      <td>4373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPORTS</th>\n",
       "      <td>4165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLACK VOICES</th>\n",
       "      <td>4070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOME &amp; LIVING</th>\n",
       "      <td>4003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE WORLDPOST</th>\n",
       "      <td>3657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEDDINGS</th>\n",
       "      <td>3645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PARENTS</th>\n",
       "      <td>3444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIVORCE</th>\n",
       "      <td>3402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IMPACT</th>\n",
       "      <td>3048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOMEN</th>\n",
       "      <td>3030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIME</th>\n",
       "      <td>2667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIA</th>\n",
       "      <td>2262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WORLD NEWS</th>\n",
       "      <td>2169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEIRD NEWS</th>\n",
       "      <td>2156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GREEN</th>\n",
       "      <td>2036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TECH</th>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TASTE</th>\n",
       "      <td>1905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RELIGION</th>\n",
       "      <td>1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCIENCE</th>\n",
       "      <td>1761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONEY</th>\n",
       "      <td>1705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STYLE</th>\n",
       "      <td>1512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARTS &amp; CULTURE</th>\n",
       "      <td>1326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENVIRONMENT</th>\n",
       "      <td>1317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WORLDPOST</th>\n",
       "      <td>1242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIFTY</th>\n",
       "      <td>1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOD NEWS</th>\n",
       "      <td>1026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CULTURE &amp; ARTS</th>\n",
       "      <td>1017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LATINO VOICES</th>\n",
       "      <td>1017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COLLEGE</th>\n",
       "      <td>921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDUCATION</th>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARTS</th>\n",
       "      <td>863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                category\n",
       "POLITICS           29131\n",
       "WELLNESS           17550\n",
       "ENTERTAINMENT      13271\n",
       "TRAVEL              9377\n",
       "STYLE & BEAUTY      9133\n",
       "PARENTING           8621\n",
       "FOOD & DRINK        6189\n",
       "QUEER VOICES        5399\n",
       "HEALTHY LIVING      5196\n",
       "BUSINESS            5056\n",
       "COMEDY              4373\n",
       "SPORTS              4165\n",
       "BLACK VOICES        4070\n",
       "HOME & LIVING       4003\n",
       "THE WORLDPOST       3657\n",
       "WEDDINGS            3645\n",
       "PARENTS             3444\n",
       "DIVORCE             3402\n",
       "IMPACT              3048\n",
       "WOMEN               3030\n",
       "CRIME               2667\n",
       "MEDIA               2262\n",
       "WORLD NEWS          2169\n",
       "WEIRD NEWS          2156\n",
       "GREEN               2036\n",
       "TECH                1984\n",
       "TASTE               1905\n",
       "RELIGION            1774\n",
       "SCIENCE             1761\n",
       "MONEY               1705\n",
       "STYLE               1512\n",
       "ARTS & CULTURE      1326\n",
       "ENVIRONMENT         1317\n",
       "WORLDPOST           1242\n",
       "FIFTY               1042\n",
       "GOOD NEWS           1026\n",
       "CULTURE & ARTS      1017\n",
       "LATINO VOICES       1017\n",
       "COLLEGE              921\n",
       "EDUCATION            891\n",
       "ARTS                 863"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories=pd.DataFrame(data['category'].value_counts())\n",
    "#41 categories\n",
    "print(categories.shape[0])\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_description</th>\n",
       "      <th>category</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>text</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>she left her husband. he killed their children...</td>\n",
       "      <td>[left, husband, killed, child, day, america]</td>\n",
       "      <td>left husband killed child day america</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>of course it has a song.</td>\n",
       "      <td>[course, song]</td>\n",
       "      <td>course song</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>the actor and his longtime girlfriend anna ebe...</td>\n",
       "      <td>[actor, longtime, girlfriend, anna, eberstein,...</td>\n",
       "      <td>actor longtime girlfriend anna eberstein tied ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>the actor gives dems an ass kicking for not fi...</td>\n",
       "      <td>[actor, dems, as, kicking, fighting, hard, don...</td>\n",
       "      <td>actor dems as kicking fighting hard donald trump</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>the dietland actress said using the bags is a ...</td>\n",
       "      <td>[dietland, actress, bag, cathartic, therapeuti...</td>\n",
       "      <td>dietland actress bag cathartic therapeutic moment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   short_description       category  \\\n",
       "0  She left her husband. He killed their children...          CRIME   \n",
       "1                           Of course it has a song.  ENTERTAINMENT   \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  ENTERTAINMENT   \n",
       "3  The actor gives Dems an ass-kicking for not fi...  ENTERTAINMENT   \n",
       "4  The \"Dietland\" actress said using the bags is ...  ENTERTAINMENT   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  she left her husband. he killed their children...   \n",
       "1                           of course it has a song.   \n",
       "2  the actor and his longtime girlfriend anna ebe...   \n",
       "3  the actor gives dems an ass kicking for not fi...   \n",
       "4  the dietland actress said using the bags is a ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0       [left, husband, killed, child, day, america]   \n",
       "1                                     [course, song]   \n",
       "2  [actor, longtime, girlfriend, anna, eberstein,...   \n",
       "3  [actor, dems, as, kicking, fighting, hard, don...   \n",
       "4  [dietland, actress, bag, cathartic, therapeuti...   \n",
       "\n",
       "                                                text  category_id  \n",
       "0              left husband killed child day america            0  \n",
       "1                                        course song            1  \n",
       "2  actor longtime girlfriend anna eberstein tied ...            1  \n",
       "3   actor dems as kicking fighting hard donald trump            1  \n",
       "4  dietland actress bag cathartic therapeutic moment            1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['category_id'] = data['category'].factorize()[0]\n",
    "category_id_df = data[['category', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'category']].values)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (178353, 41)\n",
      "(133764,) (44589,) (133764, 41) (44589, 41)\n",
      "Found 63605 unique tokens.\n",
      "Shape of X train and X validation tensor: (133764, 111) (44589, 111)\n",
      "Shape of label train and validation tensor: (133764, 41) (44589, 41)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Y = pd.get_dummies( data['category_id']).values\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "# split the dataset into training and validation datasets \n",
    "x_train, x_test, y_train, y_test = train_test_split(data['text'], Y,stratify=Y) # stratifié pour recuperer tous les labels\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "NUM_WORDS=60000\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(data.text)\n",
    "sequences_train = tokenizer.texts_to_sequences(x_train)\n",
    "sequences_valid=tokenizer.texts_to_sequences(x_test)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "X_train = pad_sequences(sequences_train)\n",
    "X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\n",
    "print('Shape of X train and X validation tensor:', X_train.shape,X_val.shape)\n",
    "print('Shape of label train and validation tensor:', y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on utilise word2vec comme wordembeding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('C:/Users/11/Desktop/m1bigdata/textminig/news-category-dataset/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\11\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 133764 samples, validate on 44589 samples\n",
      "Epoch 1/10\n",
      "133764/133764 [==============================] - ETA: 41:59 - loss: 4.4958 - acc: 0.03 - ETA: 31:52 - loss: 4.4691 - acc: 0.10 - ETA: 27:58 - loss: 4.4439 - acc: 0.12 - ETA: 26:36 - loss: 4.4179 - acc: 0.12 - ETA: 24:46 - loss: 4.3901 - acc: 0.13 - ETA: 24:03 - loss: 4.3619 - acc: 0.14 - ETA: 23:35 - loss: 4.3342 - acc: 0.14 - ETA: 22:49 - loss: 4.3082 - acc: 0.14 - ETA: 21:52 - loss: 4.2839 - acc: 0.14 - ETA: 21:00 - loss: 4.2620 - acc: 0.14 - ETA: 20:17 - loss: 4.2370 - acc: 0.15 - ETA: 19:39 - loss: 4.2187 - acc: 0.15 - ETA: 19:06 - loss: 4.2054 - acc: 0.15 - ETA: 18:35 - loss: 4.1919 - acc: 0.15 - ETA: 18:21 - loss: 4.1741 - acc: 0.15 - ETA: 17:57 - loss: 4.1622 - acc: 0.15 - ETA: 17:41 - loss: 4.1480 - acc: 0.15 - ETA: 17:28 - loss: 4.1343 - acc: 0.15 - ETA: 17:15 - loss: 4.1229 - acc: 0.15 - ETA: 16:55 - loss: 4.1064 - acc: 0.15 - ETA: 16:44 - loss: 4.0939 - acc: 0.15 - ETA: 16:32 - loss: 4.0815 - acc: 0.15 - ETA: 16:18 - loss: 4.0700 - acc: 0.15 - ETA: 16:08 - loss: 4.0573 - acc: 0.15 - ETA: 16:02 - loss: 4.0454 - acc: 0.15 - ETA: 16:00 - loss: 4.0372 - acc: 0.15 - ETA: 15:52 - loss: 4.0246 - acc: 0.15 - ETA: 15:54 - loss: 4.0161 - acc: 0.15 - ETA: 15:53 - loss: 4.0068 - acc: 0.15 - ETA: 15:51 - loss: 3.9971 - acc: 0.15 - ETA: 15:49 - loss: 3.9892 - acc: 0.15 - ETA: 15:49 - loss: 3.9796 - acc: 0.15 - ETA: 15:46 - loss: 3.9720 - acc: 0.15 - ETA: 15:33 - loss: 3.9642 - acc: 0.15 - ETA: 15:31 - loss: 3.9558 - acc: 0.15 - ETA: 15:18 - loss: 3.9478 - acc: 0.15 - ETA: 15:05 - loss: 3.9396 - acc: 0.15 - ETA: 14:53 - loss: 3.9310 - acc: 0.15 - ETA: 14:44 - loss: 3.9237 - acc: 0.15 - ETA: 14:33 - loss: 3.9176 - acc: 0.16 - ETA: 14:23 - loss: 3.9108 - acc: 0.16 - ETA: 14:13 - loss: 3.9048 - acc: 0.16 - ETA: 14:03 - loss: 3.8974 - acc: 0.16 - ETA: 13:50 - loss: 3.8908 - acc: 0.16 - ETA: 13:40 - loss: 3.8856 - acc: 0.16 - ETA: 13:30 - loss: 3.8802 - acc: 0.16 - ETA: 13:19 - loss: 3.8741 - acc: 0.16 - ETA: 13:09 - loss: 3.8672 - acc: 0.16 - ETA: 12:57 - loss: 3.8621 - acc: 0.16 - ETA: 12:49 - loss: 3.8559 - acc: 0.16 - ETA: 12:37 - loss: 3.8501 - acc: 0.16 - ETA: 12:26 - loss: 3.8431 - acc: 0.16 - ETA: 12:16 - loss: 3.8383 - acc: 0.16 - ETA: 12:08 - loss: 3.8321 - acc: 0.16 - ETA: 11:57 - loss: 3.8267 - acc: 0.16 - ETA: 11:46 - loss: 3.8213 - acc: 0.16 - ETA: 11:36 - loss: 3.8163 - acc: 0.16 - ETA: 11:27 - loss: 3.8115 - acc: 0.16 - ETA: 11:17 - loss: 3.8067 - acc: 0.16 - ETA: 11:06 - loss: 3.8006 - acc: 0.16 - ETA: 10:58 - loss: 3.7964 - acc: 0.16 - ETA: 10:51 - loss: 3.7920 - acc: 0.16 - ETA: 10:40 - loss: 3.7866 - acc: 0.16 - ETA: 10:29 - loss: 3.7816 - acc: 0.16 - ETA: 10:19 - loss: 3.7762 - acc: 0.16 - ETA: 10:08 - loss: 3.7709 - acc: 0.16 - ETA: 9:59 - loss: 3.7657 - acc: 0.1666 - ETA: 9:50 - loss: 3.7619 - acc: 0.166 - ETA: 9:40 - loss: 3.7561 - acc: 0.167 - ETA: 9:30 - loss: 3.7509 - acc: 0.168 - ETA: 9:21 - loss: 3.7466 - acc: 0.169 - ETA: 9:12 - loss: 3.7419 - acc: 0.169 - ETA: 9:02 - loss: 3.7370 - acc: 0.170 - ETA: 8:52 - loss: 3.7324 - acc: 0.171 - ETA: 8:43 - loss: 3.7282 - acc: 0.172 - ETA: 8:34 - loss: 3.7232 - acc: 0.172 - ETA: 8:24 - loss: 3.7182 - acc: 0.173 - ETA: 8:15 - loss: 3.7130 - acc: 0.174 - ETA: 8:05 - loss: 3.7082 - acc: 0.175 - ETA: 7:56 - loss: 3.7039 - acc: 0.176 - ETA: 7:46 - loss: 3.6996 - acc: 0.176 - ETA: 7:37 - loss: 3.6957 - acc: 0.177 - ETA: 7:28 - loss: 3.6911 - acc: 0.178 - ETA: 7:19 - loss: 3.6864 - acc: 0.178 - ETA: 7:09 - loss: 3.6827 - acc: 0.179 - ETA: 7:00 - loss: 3.6781 - acc: 0.179 - ETA: 6:51 - loss: 3.6744 - acc: 0.180 - ETA: 6:42 - loss: 3.6706 - acc: 0.180 - ETA: 6:33 - loss: 3.6667 - acc: 0.181 - ETA: 6:23 - loss: 3.6631 - acc: 0.181 - ETA: 6:14 - loss: 3.6590 - acc: 0.182 - ETA: 6:05 - loss: 3.6550 - acc: 0.182 - ETA: 5:56 - loss: 3.6507 - acc: 0.183 - ETA: 5:47 - loss: 3.6457 - acc: 0.183 - ETA: 5:38 - loss: 3.6412 - acc: 0.184 - ETA: 5:29 - loss: 3.6371 - acc: 0.185 - ETA: 5:20 - loss: 3.6331 - acc: 0.185 - ETA: 5:11 - loss: 3.6294 - acc: 0.186 - ETA: 5:02 - loss: 3.6258 - acc: 0.186 - ETA: 4:53 - loss: 3.6220 - acc: 0.186 - ETA: 4:44 - loss: 3.6177 - acc: 0.187 - ETA: 4:37 - loss: 3.6139 - acc: 0.187 - ETA: 4:30 - loss: 3.6096 - acc: 0.188 - ETA: 4:22 - loss: 3.6065 - acc: 0.189 - ETA: 4:14 - loss: 3.6025 - acc: 0.189 - ETA: 4:05 - loss: 3.5990 - acc: 0.189 - ETA: 3:57 - loss: 3.5950 - acc: 0.190 - ETA: 3:49 - loss: 3.5910 - acc: 0.190 - ETA: 3:42 - loss: 3.5870 - acc: 0.191 - ETA: 3:33 - loss: 3.5837 - acc: 0.191 - ETA: 3:25 - loss: 3.5795 - acc: 0.192 - ETA: 3:16 - loss: 3.5759 - acc: 0.192 - ETA: 3:07 - loss: 3.5722 - acc: 0.193 - ETA: 2:58 - loss: 3.5683 - acc: 0.194 - ETA: 2:49 - loss: 3.5646 - acc: 0.194 - ETA: 2:40 - loss: 3.5613 - acc: 0.195 - ETA: 2:31 - loss: 3.5579 - acc: 0.195 - ETA: 2:22 - loss: 3.5540 - acc: 0.196 - ETA: 2:13 - loss: 3.5502 - acc: 0.196 - ETA: 2:04 - loss: 3.5469 - acc: 0.197 - ETA: 1:55 - loss: 3.5437 - acc: 0.197 - ETA: 1:46 - loss: 3.5402 - acc: 0.198 - ETA: 1:37 - loss: 3.5373 - acc: 0.198 - ETA: 1:28 - loss: 3.5337 - acc: 0.198 - ETA: 1:19 - loss: 3.5308 - acc: 0.199 - ETA: 1:10 - loss: 3.5276 - acc: 0.199 - ETA: 1:01 - loss: 3.5241 - acc: 0.200 - ETA: 52s - loss: 3.5211 - acc: 0.200 - ETA: 43s - loss: 3.5184 - acc: 0.20 - ETA: 34s - loss: 3.5159 - acc: 0.20 - ETA: 25s - loss: 3.5130 - acc: 0.20 - ETA: 16s - loss: 3.5102 - acc: 0.20 - ETA: 6s - loss: 3.5072 - acc: 0.2024 - 1366s 10ms/step - loss: 3.5052 - acc: 0.2026 - val_loss: 3.1026 - val_acc: 0.2641\n",
      "Epoch 2/10\n",
      "133764/133764 [==============================] - ETA: 30:00 - loss: 3.1247 - acc: 0.25 - ETA: 28:33 - loss: 3.0952 - acc: 0.26 - ETA: 28:44 - loss: 3.0745 - acc: 0.27 - ETA: 27:37 - loss: 3.0921 - acc: 0.27 - ETA: 27:08 - loss: 3.0976 - acc: 0.26 - ETA: 27:09 - loss: 3.1028 - acc: 0.26 - ETA: 25:31 - loss: 3.1076 - acc: 0.26 - ETA: 24:17 - loss: 3.1008 - acc: 0.26 - ETA: 23:34 - loss: 3.0874 - acc: 0.26 - ETA: 22:40 - loss: 3.1003 - acc: 0.26 - ETA: 22:30 - loss: 3.1072 - acc: 0.26 - ETA: 22:23 - loss: 3.1016 - acc: 0.26 - ETA: 22:28 - loss: 3.1027 - acc: 0.26 - ETA: 22:33 - loss: 3.1008 - acc: 0.26 - ETA: 22:34 - loss: 3.1015 - acc: 0.26 - ETA: 22:22 - loss: 3.0961 - acc: 0.26 - ETA: 22:21 - loss: 3.0977 - acc: 0.26 - ETA: 22:15 - loss: 3.0984 - acc: 0.26 - ETA: 22:08 - loss: 3.0967 - acc: 0.26 - ETA: 21:53 - loss: 3.0945 - acc: 0.26 - ETA: 21:37 - loss: 3.0930 - acc: 0.26 - ETA: 21:18 - loss: 3.0947 - acc: 0.26 - ETA: 21:00 - loss: 3.0959 - acc: 0.26 - ETA: 20:36 - loss: 3.0968 - acc: 0.26 - ETA: 20:21 - loss: 3.0953 - acc: 0.26 - ETA: 20:12 - loss: 3.0914 - acc: 0.26 - ETA: 19:50 - loss: 3.0911 - acc: 0.26 - ETA: 19:37 - loss: 3.0890 - acc: 0.26 - ETA: 19:16 - loss: 3.0854 - acc: 0.26 - ETA: 19:02 - loss: 3.0827 - acc: 0.26 - ETA: 18:47 - loss: 3.0819 - acc: 0.26 - ETA: 18:36 - loss: 3.0787 - acc: 0.26 - ETA: 18:18 - loss: 3.0794 - acc: 0.26 - ETA: 18:08 - loss: 3.0788 - acc: 0.26 - ETA: 17:51 - loss: 3.0767 - acc: 0.26 - ETA: 17:38 - loss: 3.0767 - acc: 0.26 - ETA: 17:25 - loss: 3.0771 - acc: 0.26 - ETA: 17:14 - loss: 3.0768 - acc: 0.26 - ETA: 16:58 - loss: 3.0755 - acc: 0.26 - ETA: 16:45 - loss: 3.0746 - acc: 0.26 - ETA: 16:32 - loss: 3.0737 - acc: 0.26 - ETA: 16:21 - loss: 3.0711 - acc: 0.26 - ETA: 16:13 - loss: 3.0716 - acc: 0.26 - ETA: 16:01 - loss: 3.0712 - acc: 0.26 - ETA: 15:46 - loss: 3.0715 - acc: 0.26 - ETA: 15:37 - loss: 3.0728 - acc: 0.26 - ETA: 15:29 - loss: 3.0728 - acc: 0.26 - ETA: 15:15 - loss: 3.0717 - acc: 0.26 - ETA: 15:04 - loss: 3.0710 - acc: 0.26 - ETA: 14:53 - loss: 3.0703 - acc: 0.26 - ETA: 14:39 - loss: 3.0700 - acc: 0.26 - ETA: 14:30 - loss: 3.0693 - acc: 0.26 - ETA: 14:19 - loss: 3.0686 - acc: 0.26 - ETA: 14:09 - loss: 3.0676 - acc: 0.26 - ETA: 13:55 - loss: 3.0670 - acc: 0.26 - ETA: 13:44 - loss: 3.0650 - acc: 0.26 - ETA: 13:34 - loss: 3.0654 - acc: 0.26 - ETA: 13:23 - loss: 3.0645 - acc: 0.26 - ETA: 13:13 - loss: 3.0638 - acc: 0.26 - ETA: 13:01 - loss: 3.0647 - acc: 0.26 - ETA: 12:48 - loss: 3.0653 - acc: 0.26 - ETA: 12:37 - loss: 3.0658 - acc: 0.26 - ETA: 12:29 - loss: 3.0653 - acc: 0.26 - ETA: 12:16 - loss: 3.0653 - acc: 0.26 - ETA: 12:04 - loss: 3.0653 - acc: 0.26 - ETA: 11:54 - loss: 3.0645 - acc: 0.26 - ETA: 11:42 - loss: 3.0646 - acc: 0.26 - ETA: 11:32 - loss: 3.0646 - acc: 0.26 - ETA: 11:21 - loss: 3.0644 - acc: 0.26 - ETA: 11:11 - loss: 3.0639 - acc: 0.26 - ETA: 11:00 - loss: 3.0630 - acc: 0.26 - ETA: 10:50 - loss: 3.0634 - acc: 0.26 - ETA: 10:39 - loss: 3.0632 - acc: 0.26 - ETA: 10:26 - loss: 3.0630 - acc: 0.26 - ETA: 10:15 - loss: 3.0623 - acc: 0.26 - ETA: 10:05 - loss: 3.0607 - acc: 0.26 - ETA: 9:53 - loss: 3.0606 - acc: 0.2676 - ETA: 9:43 - loss: 3.0599 - acc: 0.267 - ETA: 9:31 - loss: 3.0598 - acc: 0.267 - ETA: 9:21 - loss: 3.0593 - acc: 0.267 - ETA: 9:10 - loss: 3.0589 - acc: 0.267 - ETA: 8:59 - loss: 3.0590 - acc: 0.267 - ETA: 8:49 - loss: 3.0589 - acc: 0.267 - ETA: 8:37 - loss: 3.0591 - acc: 0.267 - ETA: 8:27 - loss: 3.0597 - acc: 0.267 - ETA: 8:16 - loss: 3.0593 - acc: 0.266 - ETA: 8:05 - loss: 3.0582 - acc: 0.267 - ETA: 7:54 - loss: 3.0567 - acc: 0.267 - ETA: 7:43 - loss: 3.0560 - acc: 0.267 - ETA: 7:33 - loss: 3.0558 - acc: 0.267 - ETA: 7:23 - loss: 3.0550 - acc: 0.267 - ETA: 7:11 - loss: 3.0546 - acc: 0.267 - ETA: 7:01 - loss: 3.0540 - acc: 0.268 - ETA: 6:50 - loss: 3.0543 - acc: 0.267 - ETA: 6:39 - loss: 3.0543 - acc: 0.267 - ETA: 6:29 - loss: 3.0536 - acc: 0.267 - ETA: 6:18 - loss: 3.0534 - acc: 0.267 - ETA: 6:07 - loss: 3.0526 - acc: 0.268 - ETA: 5:57 - loss: 3.0525 - acc: 0.268 - ETA: 5:46 - loss: 3.0527 - acc: 0.268 - ETA: 5:35 - loss: 3.0526 - acc: 0.268 - ETA: 5:25 - loss: 3.0516 - acc: 0.268 - ETA: 5:15 - loss: 3.0512 - acc: 0.268 - ETA: 5:05 - loss: 3.0511 - acc: 0.268 - ETA: 4:54 - loss: 3.0511 - acc: 0.268 - ETA: 4:43 - loss: 3.0513 - acc: 0.267 - ETA: 4:33 - loss: 3.0510 - acc: 0.267 - ETA: 4:23 - loss: 3.0499 - acc: 0.268 - ETA: 4:12 - loss: 3.0493 - acc: 0.268 - ETA: 4:02 - loss: 3.0482 - acc: 0.268 - ETA: 3:52 - loss: 3.0481 - acc: 0.268 - ETA: 3:42 - loss: 3.0483 - acc: 0.268 - ETA: 3:32 - loss: 3.0472 - acc: 0.268 - ETA: 3:21 - loss: 3.0472 - acc: 0.268 - ETA: 3:11 - loss: 3.0467 - acc: 0.268 - ETA: 3:01 - loss: 3.0464 - acc: 0.268 - ETA: 2:51 - loss: 3.0457 - acc: 0.268 - ETA: 2:41 - loss: 3.0461 - acc: 0.268 - ETA: 2:31 - loss: 3.0461 - acc: 0.268 - ETA: 2:20 - loss: 3.0464 - acc: 0.268 - ETA: 2:10 - loss: 3.0464 - acc: 0.268 - ETA: 2:00 - loss: 3.0460 - acc: 0.268 - ETA: 1:49 - loss: 3.0459 - acc: 0.268 - ETA: 1:39 - loss: 3.0463 - acc: 0.268 - ETA: 1:29 - loss: 3.0460 - acc: 0.268 - ETA: 1:19 - loss: 3.0462 - acc: 0.268 - ETA: 1:08 - loss: 3.0458 - acc: 0.268 - ETA: 58s - loss: 3.0456 - acc: 0.268 - ETA: 48s - loss: 3.0454 - acc: 0.26 - ETA: 38s - loss: 3.0446 - acc: 0.26 - ETA: 28s - loss: 3.0442 - acc: 0.26 - ETA: 17s - loss: 3.0443 - acc: 0.26 - ETA: 7s - loss: 3.0438 - acc: 0.2682 - 1497s 11ms/step - loss: 3.0437 - acc: 0.2682 - val_loss: 2.9917 - val_acc: 0.2724\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133764/133764 [==============================] - ETA: 22:37 - loss: 2.9519 - acc: 0.29 - ETA: 22:39 - loss: 2.9966 - acc: 0.26 - ETA: 21:35 - loss: 2.9457 - acc: 0.28 - ETA: 22:19 - loss: 2.9614 - acc: 0.28 - ETA: 21:21 - loss: 2.9629 - acc: 0.28 - ETA: 21:06 - loss: 2.9708 - acc: 0.27 - ETA: 20:56 - loss: 2.9730 - acc: 0.27 - ETA: 20:31 - loss: 2.9716 - acc: 0.27 - ETA: 20:18 - loss: 2.9702 - acc: 0.27 - ETA: 20:19 - loss: 2.9720 - acc: 0.27 - ETA: 20:42 - loss: 2.9651 - acc: 0.27 - ETA: 20:46 - loss: 2.9721 - acc: 0.27 - ETA: 20:50 - loss: 2.9761 - acc: 0.27 - ETA: 20:22 - loss: 2.9780 - acc: 0.27 - ETA: 20:02 - loss: 2.9791 - acc: 0.27 - ETA: 19:49 - loss: 2.9753 - acc: 0.27 - ETA: 19:40 - loss: 2.9777 - acc: 0.27 - ETA: 19:20 - loss: 2.9775 - acc: 0.27 - ETA: 19:09 - loss: 2.9746 - acc: 0.27 - ETA: 18:58 - loss: 2.9746 - acc: 0.27 - ETA: 18:50 - loss: 2.9749 - acc: 0.27 - ETA: 18:51 - loss: 2.9769 - acc: 0.27 - ETA: 18:41 - loss: 2.9776 - acc: 0.27 - ETA: 18:24 - loss: 2.9736 - acc: 0.27 - ETA: 18:15 - loss: 2.9703 - acc: 0.27 - ETA: 18:05 - loss: 2.9685 - acc: 0.27 - ETA: 17:56 - loss: 2.9681 - acc: 0.27 - ETA: 17:47 - loss: 2.9699 - acc: 0.27 - ETA: 17:44 - loss: 2.9684 - acc: 0.27 - ETA: 17:36 - loss: 2.9699 - acc: 0.27 - ETA: 17:31 - loss: 2.9702 - acc: 0.27 - ETA: 17:16 - loss: 2.9703 - acc: 0.27 - ETA: 17:03 - loss: 2.9707 - acc: 0.27 - ETA: 16:54 - loss: 2.9698 - acc: 0.27 - ETA: 16:45 - loss: 2.9694 - acc: 0.27 - ETA: 16:37 - loss: 2.9706 - acc: 0.27 - ETA: 16:23 - loss: 2.9695 - acc: 0.27 - ETA: 16:12 - loss: 2.9707 - acc: 0.27 - ETA: 16:04 - loss: 2.9713 - acc: 0.27 - ETA: 15:54 - loss: 2.9710 - acc: 0.27 - ETA: 15:43 - loss: 2.9688 - acc: 0.27 - ETA: 15:31 - loss: 2.9691 - acc: 0.27 - ETA: 15:23 - loss: 2.9693 - acc: 0.27 - ETA: 15:14 - loss: 2.9712 - acc: 0.27 - ETA: 15:01 - loss: 2.9694 - acc: 0.27 - ETA: 14:51 - loss: 2.9692 - acc: 0.28 - ETA: 14:45 - loss: 2.9682 - acc: 0.28 - ETA: 14:32 - loss: 2.9689 - acc: 0.28 - ETA: 14:21 - loss: 2.9681 - acc: 0.28 - ETA: 14:12 - loss: 2.9671 - acc: 0.28 - ETA: 14:02 - loss: 2.9668 - acc: 0.28 - ETA: 13:52 - loss: 2.9671 - acc: 0.28 - ETA: 13:42 - loss: 2.9659 - acc: 0.28 - ETA: 13:32 - loss: 2.9645 - acc: 0.28 - ETA: 13:21 - loss: 2.9637 - acc: 0.28 - ETA: 13:12 - loss: 2.9641 - acc: 0.28 - ETA: 13:01 - loss: 2.9642 - acc: 0.28 - ETA: 12:49 - loss: 2.9642 - acc: 0.28 - ETA: 12:39 - loss: 2.9648 - acc: 0.28 - ETA: 12:31 - loss: 2.9642 - acc: 0.28 - ETA: 12:22 - loss: 2.9644 - acc: 0.28 - ETA: 12:12 - loss: 2.9644 - acc: 0.28 - ETA: 12:00 - loss: 2.9647 - acc: 0.28 - ETA: 11:50 - loss: 2.9647 - acc: 0.28 - ETA: 11:39 - loss: 2.9648 - acc: 0.28 - ETA: 11:28 - loss: 2.9657 - acc: 0.28 - ETA: 11:20 - loss: 2.9654 - acc: 0.28 - ETA: 11:08 - loss: 2.9650 - acc: 0.28 - ETA: 10:58 - loss: 2.9650 - acc: 0.28 - ETA: 10:47 - loss: 2.9640 - acc: 0.28 - ETA: 10:36 - loss: 2.9639 - acc: 0.28 - ETA: 10:29 - loss: 2.9641 - acc: 0.28 - ETA: 10:18 - loss: 2.9642 - acc: 0.28 - ETA: 10:07 - loss: 2.9644 - acc: 0.28 - ETA: 9:57 - loss: 2.9649 - acc: 0.2817 - ETA: 9:45 - loss: 2.9662 - acc: 0.281 - ETA: 9:36 - loss: 2.9663 - acc: 0.281 - ETA: 9:28 - loss: 2.9665 - acc: 0.281 - ETA: 9:16 - loss: 2.9666 - acc: 0.281 - ETA: 9:06 - loss: 2.9661 - acc: 0.281 - ETA: 8:58 - loss: 2.9661 - acc: 0.281 - ETA: 8:46 - loss: 2.9665 - acc: 0.281 - ETA: 8:35 - loss: 2.9670 - acc: 0.281 - ETA: 8:24 - loss: 2.9668 - acc: 0.281 - ETA: 8:13 - loss: 2.9673 - acc: 0.281 - ETA: 8:03 - loss: 2.9676 - acc: 0.281 - ETA: 7:52 - loss: 2.9668 - acc: 0.281 - ETA: 7:42 - loss: 2.9665 - acc: 0.281 - ETA: 7:34 - loss: 2.9671 - acc: 0.281 - ETA: 7:25 - loss: 2.9667 - acc: 0.281 - ETA: 7:15 - loss: 2.9668 - acc: 0.281 - ETA: 7:03 - loss: 2.9663 - acc: 0.281 - ETA: 6:52 - loss: 2.9663 - acc: 0.281 - ETA: 6:42 - loss: 2.9661 - acc: 0.282 - ETA: 6:31 - loss: 2.9658 - acc: 0.282 - ETA: 6:21 - loss: 2.9663 - acc: 0.282 - ETA: 6:11 - loss: 2.9660 - acc: 0.282 - ETA: 6:00 - loss: 2.9651 - acc: 0.282 - ETA: 5:49 - loss: 2.9649 - acc: 0.282 - ETA: 5:39 - loss: 2.9652 - acc: 0.282 - ETA: 5:29 - loss: 2.9651 - acc: 0.282 - ETA: 5:19 - loss: 2.9642 - acc: 0.282 - ETA: 5:08 - loss: 2.9634 - acc: 0.283 - ETA: 4:58 - loss: 2.9626 - acc: 0.283 - ETA: 4:48 - loss: 2.9617 - acc: 0.283 - ETA: 4:38 - loss: 2.9619 - acc: 0.283 - ETA: 4:27 - loss: 2.9615 - acc: 0.283 - ETA: 4:17 - loss: 2.9608 - acc: 0.283 - ETA: 4:07 - loss: 2.9602 - acc: 0.284 - ETA: 3:56 - loss: 2.9595 - acc: 0.284 - ETA: 3:46 - loss: 2.9594 - acc: 0.284 - ETA: 3:36 - loss: 2.9593 - acc: 0.284 - ETA: 3:26 - loss: 2.9594 - acc: 0.284 - ETA: 3:15 - loss: 2.9589 - acc: 0.284 - ETA: 3:05 - loss: 2.9584 - acc: 0.284 - ETA: 2:55 - loss: 2.9587 - acc: 0.284 - ETA: 2:45 - loss: 2.9585 - acc: 0.284 - ETA: 2:35 - loss: 2.9589 - acc: 0.284 - ETA: 2:25 - loss: 2.9594 - acc: 0.284 - ETA: 2:15 - loss: 2.9588 - acc: 0.284 - ETA: 2:05 - loss: 2.9587 - acc: 0.284 - ETA: 1:55 - loss: 2.9579 - acc: 0.285 - ETA: 1:45 - loss: 2.9584 - acc: 0.285 - ETA: 1:35 - loss: 2.9579 - acc: 0.285 - ETA: 1:25 - loss: 2.9571 - acc: 0.285 - ETA: 1:15 - loss: 2.9568 - acc: 0.285 - ETA: 1:06 - loss: 2.9569 - acc: 0.285 - ETA: 56s - loss: 2.9567 - acc: 0.285 - ETA: 46s - loss: 2.9562 - acc: 0.28 - ETA: 36s - loss: 2.9566 - acc: 0.28 - ETA: 26s - loss: 2.9570 - acc: 0.28 - ETA: 17s - loss: 2.9575 - acc: 0.28 - ETA: 7s - loss: 2.9576 - acc: 0.2860 - 1411s 11ms/step - loss: 2.9574 - acc: 0.2861 - val_loss: 2.9335 - val_acc: 0.2977\n",
      "Epoch 4/10\n",
      "133764/133764 [==============================] - ETA: 20:53 - loss: 2.7921 - acc: 0.35 - ETA: 20:30 - loss: 2.8245 - acc: 0.33 - ETA: 20:11 - loss: 2.8534 - acc: 0.32 - ETA: 19:07 - loss: 2.8787 - acc: 0.32 - ETA: 19:11 - loss: 2.8850 - acc: 0.31 - ETA: 19:08 - loss: 2.8950 - acc: 0.31 - ETA: 19:07 - loss: 2.8870 - acc: 0.31 - ETA: 18:37 - loss: 2.8883 - acc: 0.31 - ETA: 18:23 - loss: 2.8932 - acc: 0.31 - ETA: 18:11 - loss: 2.8924 - acc: 0.31 - ETA: 17:59 - loss: 2.8949 - acc: 0.31 - ETA: 17:34 - loss: 2.8965 - acc: 0.31 - ETA: 17:22 - loss: 2.8949 - acc: 0.31 - ETA: 17:12 - loss: 2.8921 - acc: 0.31 - ETA: 17:03 - loss: 2.8923 - acc: 0.31 - ETA: 16:51 - loss: 2.8943 - acc: 0.31 - ETA: 16:34 - loss: 2.8976 - acc: 0.31 - ETA: 16:26 - loss: 2.8975 - acc: 0.31 - ETA: 16:09 - loss: 2.8988 - acc: 0.30 - ETA: 16:01 - loss: 2.8972 - acc: 0.30 - ETA: 15:53 - loss: 2.8944 - acc: 0.31 - ETA: 15:46 - loss: 2.8938 - acc: 0.31 - ETA: 15:32 - loss: 2.8934 - acc: 0.31 - ETA: 15:24 - loss: 2.8937 - acc: 0.31 - ETA: 15:15 - loss: 2.8924 - acc: 0.31 - ETA: 15:07 - loss: 2.8925 - acc: 0.31 - ETA: 14:59 - loss: 2.8921 - acc: 0.31 - ETA: 14:47 - loss: 2.8906 - acc: 0.31 - ETA: 14:39 - loss: 2.8906 - acc: 0.31 - ETA: 14:32 - loss: 2.8879 - acc: 0.31 - ETA: 14:24 - loss: 2.8866 - acc: 0.31 - ETA: 14:16 - loss: 2.8861 - acc: 0.31 - ETA: 14:04 - loss: 2.8874 - acc: 0.31 - ETA: 13:56 - loss: 2.8868 - acc: 0.31 - ETA: 13:48 - loss: 2.8876 - acc: 0.31 - ETA: 13:37 - loss: 2.8881 - acc: 0.31 - ETA: 13:29 - loss: 2.8887 - acc: 0.31 - ETA: 13:22 - loss: 2.8870 - acc: 0.31 - ETA: 13:15 - loss: 2.8869 - acc: 0.31 - ETA: 13:05 - loss: 2.8858 - acc: 0.31 - ETA: 13:00 - loss: 2.8859 - acc: 0.31 - ETA: 12:49 - loss: 2.8863 - acc: 0.31 - ETA: 12:42 - loss: 2.8876 - acc: 0.31 - ETA: 12:35 - loss: 2.8879 - acc: 0.31 - ETA: 12:29 - loss: 2.8878 - acc: 0.31 - ETA: 12:18 - loss: 2.8879 - acc: 0.31 - ETA: 12:10 - loss: 2.8888 - acc: 0.31 - ETA: 12:02 - loss: 2.8893 - acc: 0.31 - ETA: 11:53 - loss: 2.8915 - acc: 0.31 - ETA: 11:43 - loss: 2.8909 - acc: 0.31 - ETA: 11:35 - loss: 2.8902 - acc: 0.31 - ETA: 11:27 - loss: 2.8888 - acc: 0.31 - ETA: 11:17 - loss: 2.8885 - acc: 0.31 - ETA: 11:09 - loss: 2.8889 - acc: 0.31 - ETA: 11:01 - loss: 2.8890 - acc: 0.31 - ETA: 10:51 - loss: 2.8886 - acc: 0.31 - ETA: 10:42 - loss: 2.8868 - acc: 0.31 - ETA: 10:35 - loss: 2.8879 - acc: 0.31 - ETA: 10:25 - loss: 2.8886 - acc: 0.31 - ETA: 10:17 - loss: 2.8876 - acc: 0.31 - ETA: 10:09 - loss: 2.8867 - acc: 0.31 - ETA: 10:01 - loss: 2.8859 - acc: 0.31 - ETA: 9:51 - loss: 2.8857 - acc: 0.3130 - ETA: 9:44 - loss: 2.8866 - acc: 0.312 - ETA: 9:35 - loss: 2.8864 - acc: 0.313 - ETA: 9:26 - loss: 2.8862 - acc: 0.313 - ETA: 9:18 - loss: 2.8848 - acc: 0.313 - ETA: 9:10 - loss: 2.8843 - acc: 0.313 - ETA: 9:01 - loss: 2.8837 - acc: 0.313 - ETA: 8:52 - loss: 2.8848 - acc: 0.313 - ETA: 8:44 - loss: 2.8846 - acc: 0.313 - ETA: 8:36 - loss: 2.8837 - acc: 0.313 - ETA: 8:26 - loss: 2.8820 - acc: 0.314 - ETA: 8:18 - loss: 2.8823 - acc: 0.314 - ETA: 8:10 - loss: 2.8827 - acc: 0.314 - ETA: 8:01 - loss: 2.8829 - acc: 0.314 - ETA: 7:52 - loss: 2.8822 - acc: 0.314 - ETA: 7:44 - loss: 2.8815 - acc: 0.314 - ETA: 7:36 - loss: 2.8820 - acc: 0.314 - ETA: 7:27 - loss: 2.8810 - acc: 0.315 - ETA: 7:19 - loss: 2.8807 - acc: 0.315 - ETA: 7:10 - loss: 2.8807 - acc: 0.315 - ETA: 7:02 - loss: 2.8804 - acc: 0.315 - ETA: 6:53 - loss: 2.8800 - acc: 0.315 - ETA: 6:45 - loss: 2.8796 - acc: 0.315 - ETA: 6:37 - loss: 2.8788 - acc: 0.315 - ETA: 6:28 - loss: 2.8786 - acc: 0.315 - ETA: 6:20 - loss: 2.8793 - acc: 0.315 - ETA: 6:11 - loss: 2.8789 - acc: 0.315 - ETA: 6:03 - loss: 2.8793 - acc: 0.315 - ETA: 5:54 - loss: 2.8799 - acc: 0.315 - ETA: 5:46 - loss: 2.8804 - acc: 0.314 - ETA: 5:38 - loss: 2.8804 - acc: 0.314 - ETA: 5:29 - loss: 2.8804 - acc: 0.314 - ETA: 5:21 - loss: 2.8804 - acc: 0.314 - ETA: 5:13 - loss: 2.8808 - acc: 0.314 - ETA: 5:04 - loss: 2.8812 - acc: 0.314 - ETA: 4:56 - loss: 2.8804 - acc: 0.315 - ETA: 4:48 - loss: 2.8800 - acc: 0.315 - ETA: 4:39 - loss: 2.8793 - acc: 0.315 - ETA: 4:31 - loss: 2.8790 - acc: 0.315 - ETA: 4:22 - loss: 2.8794 - acc: 0.315 - ETA: 4:14 - loss: 2.8791 - acc: 0.315 - ETA: 4:06 - loss: 2.8798 - acc: 0.315 - ETA: 3:57 - loss: 2.8798 - acc: 0.315 - ETA: 3:49 - loss: 2.8796 - acc: 0.315 - ETA: 3:41 - loss: 2.8792 - acc: 0.315 - ETA: 3:33 - loss: 2.8789 - acc: 0.315 - ETA: 3:24 - loss: 2.8786 - acc: 0.315 - ETA: 3:16 - loss: 2.8789 - acc: 0.315 - ETA: 3:08 - loss: 2.8789 - acc: 0.315 - ETA: 3:00 - loss: 2.8789 - acc: 0.315 - ETA: 2:52 - loss: 2.8788 - acc: 0.315 - ETA: 2:44 - loss: 2.8788 - acc: 0.315 - ETA: 2:35 - loss: 2.8790 - acc: 0.315 - ETA: 2:27 - loss: 2.8792 - acc: 0.315 - ETA: 2:19 - loss: 2.8790 - acc: 0.315 - ETA: 2:11 - loss: 2.8787 - acc: 0.315 - ETA: 2:02 - loss: 2.8788 - acc: 0.315 - ETA: 1:54 - loss: 2.8778 - acc: 0.316 - ETA: 1:46 - loss: 2.8775 - acc: 0.316 - ETA: 1:38 - loss: 2.8776 - acc: 0.316 - ETA: 1:30 - loss: 2.8775 - acc: 0.316 - ETA: 1:21 - loss: 2.8774 - acc: 0.316 - ETA: 1:13 - loss: 2.8772 - acc: 0.316 - ETA: 1:05 - loss: 2.8768 - acc: 0.316 - ETA: 56s - loss: 2.8766 - acc: 0.316 - ETA: 48s - loss: 2.8766 - acc: 0.31 - ETA: 40s - loss: 2.8765 - acc: 0.31 - ETA: 31s - loss: 2.8769 - acc: 0.31 - ETA: 23s - loss: 2.8764 - acc: 0.31 - ETA: 15s - loss: 2.8765 - acc: 0.31 - ETA: 6s - loss: 2.8761 - acc: 0.3165 - 1284s 10ms/step - loss: 2.8762 - acc: 0.3165 - val_loss: 2.8603 - val_acc: 0.3176\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133764/133764 [==============================] - ETA: 22:08 - loss: 2.7895 - acc: 0.33 - ETA: 20:39 - loss: 2.8151 - acc: 0.32 - ETA: 20:53 - loss: 2.8304 - acc: 0.32 - ETA: 21:06 - loss: 2.8308 - acc: 0.31 - ETA: 21:18 - loss: 2.8250 - acc: 0.32 - ETA: 21:17 - loss: 2.8223 - acc: 0.32 - ETA: 21:10 - loss: 2.8056 - acc: 0.32 - ETA: 20:39 - loss: 2.8072 - acc: 0.32 - ETA: 20:54 - loss: 2.8070 - acc: 0.32 - ETA: 20:51 - loss: 2.8092 - acc: 0.32 - ETA: 20:26 - loss: 2.8120 - acc: 0.32 - ETA: 20:33 - loss: 2.8061 - acc: 0.33 - ETA: 20:08 - loss: 2.8093 - acc: 0.33 - ETA: 19:59 - loss: 2.8091 - acc: 0.33 - ETA: 19:37 - loss: 2.8133 - acc: 0.32 - ETA: 19:40 - loss: 2.8165 - acc: 0.32 - ETA: 19:21 - loss: 2.8164 - acc: 0.32 - ETA: 19:13 - loss: 2.8179 - acc: 0.32 - ETA: 19:02 - loss: 2.8140 - acc: 0.33 - ETA: 18:51 - loss: 2.8137 - acc: 0.33 - ETA: 18:34 - loss: 2.8135 - acc: 0.33 - ETA: 18:22 - loss: 2.8124 - acc: 0.33 - ETA: 18:12 - loss: 2.8133 - acc: 0.33 - ETA: 18:01 - loss: 2.8156 - acc: 0.33 - ETA: 17:48 - loss: 2.8183 - acc: 0.32 - ETA: 17:37 - loss: 2.8180 - acc: 0.32 - ETA: 17:27 - loss: 2.8216 - acc: 0.32 - ETA: 17:19 - loss: 2.8249 - acc: 0.32 - ETA: 17:09 - loss: 2.8231 - acc: 0.32 - ETA: 17:10 - loss: 2.8244 - acc: 0.32 - ETA: 17:03 - loss: 2.8220 - acc: 0.32 - ETA: 17:01 - loss: 2.8230 - acc: 0.32 - ETA: 16:50 - loss: 2.8212 - acc: 0.32 - ETA: 16:39 - loss: 2.8185 - acc: 0.32 - ETA: 16:32 - loss: 2.8186 - acc: 0.32 - ETA: 16:23 - loss: 2.8184 - acc: 0.32 - ETA: 16:14 - loss: 2.8176 - acc: 0.32 - ETA: 16:04 - loss: 2.8177 - acc: 0.32 - ETA: 15:55 - loss: 2.8171 - acc: 0.32 - ETA: 15:46 - loss: 2.8165 - acc: 0.32 - ETA: 15:34 - loss: 2.8162 - acc: 0.32 - ETA: 15:24 - loss: 2.8161 - acc: 0.33 - ETA: 15:16 - loss: 2.8149 - acc: 0.33 - ETA: 15:06 - loss: 2.8143 - acc: 0.33 - ETA: 14:53 - loss: 2.8142 - acc: 0.33 - ETA: 14:43 - loss: 2.8142 - acc: 0.33 - ETA: 14:36 - loss: 2.8147 - acc: 0.33 - ETA: 14:23 - loss: 2.8125 - acc: 0.33 - ETA: 14:15 - loss: 2.8121 - acc: 0.33 - ETA: 14:05 - loss: 2.8128 - acc: 0.33 - ETA: 13:56 - loss: 2.8126 - acc: 0.33 - ETA: 13:44 - loss: 2.8128 - acc: 0.33 - ETA: 13:33 - loss: 2.8120 - acc: 0.33 - ETA: 13:21 - loss: 2.8113 - acc: 0.33 - ETA: 13:10 - loss: 2.8103 - acc: 0.33 - ETA: 13:00 - loss: 2.8106 - acc: 0.33 - ETA: 12:50 - loss: 2.8099 - acc: 0.33 - ETA: 12:38 - loss: 2.8100 - acc: 0.33 - ETA: 12:30 - loss: 2.8108 - acc: 0.33 - ETA: 12:18 - loss: 2.8107 - acc: 0.33 - ETA: 12:08 - loss: 2.8100 - acc: 0.33 - ETA: 11:57 - loss: 2.8096 - acc: 0.33 - ETA: 11:49 - loss: 2.8092 - acc: 0.33 - ETA: 11:38 - loss: 2.8085 - acc: 0.33 - ETA: 11:30 - loss: 2.8088 - acc: 0.33 - ETA: 11:18 - loss: 2.8083 - acc: 0.33 - ETA: 11:08 - loss: 2.8089 - acc: 0.33 - ETA: 11:00 - loss: 2.8096 - acc: 0.33 - ETA: 10:50 - loss: 2.8095 - acc: 0.33 - ETA: 10:39 - loss: 2.8102 - acc: 0.33 - ETA: 10:29 - loss: 2.8099 - acc: 0.33 - ETA: 10:18 - loss: 2.8097 - acc: 0.33 - ETA: 10:08 - loss: 2.8091 - acc: 0.33 - ETA: 9:57 - loss: 2.8093 - acc: 0.3341 - ETA: 9:46 - loss: 2.8088 - acc: 0.334 - ETA: 9:38 - loss: 2.8092 - acc: 0.334 - ETA: 9:27 - loss: 2.8089 - acc: 0.334 - ETA: 9:16 - loss: 2.8087 - acc: 0.334 - ETA: 9:06 - loss: 2.8091 - acc: 0.334 - ETA: 8:55 - loss: 2.8108 - acc: 0.333 - ETA: 8:46 - loss: 2.8108 - acc: 0.334 - ETA: 8:35 - loss: 2.8105 - acc: 0.334 - ETA: 8:24 - loss: 2.8105 - acc: 0.334 - ETA: 8:13 - loss: 2.8114 - acc: 0.334 - ETA: 8:03 - loss: 2.8114 - acc: 0.334 - ETA: 7:51 - loss: 2.8110 - acc: 0.334 - ETA: 7:41 - loss: 2.8111 - acc: 0.334 - ETA: 7:30 - loss: 2.8113 - acc: 0.334 - ETA: 7:19 - loss: 2.8112 - acc: 0.334 - ETA: 7:08 - loss: 2.8110 - acc: 0.334 - ETA: 6:58 - loss: 2.8105 - acc: 0.334 - ETA: 6:47 - loss: 2.8103 - acc: 0.334 - ETA: 6:37 - loss: 2.8105 - acc: 0.334 - ETA: 6:26 - loss: 2.8104 - acc: 0.334 - ETA: 6:16 - loss: 2.8098 - acc: 0.334 - ETA: 6:06 - loss: 2.8103 - acc: 0.334 - ETA: 5:55 - loss: 2.8108 - acc: 0.334 - ETA: 5:45 - loss: 2.8109 - acc: 0.334 - ETA: 5:35 - loss: 2.8109 - acc: 0.334 - ETA: 5:25 - loss: 2.8102 - acc: 0.334 - ETA: 5:15 - loss: 2.8109 - acc: 0.334 - ETA: 5:05 - loss: 2.8106 - acc: 0.334 - ETA: 4:55 - loss: 2.8103 - acc: 0.334 - ETA: 4:45 - loss: 2.8095 - acc: 0.335 - ETA: 4:35 - loss: 2.8094 - acc: 0.335 - ETA: 4:25 - loss: 2.8096 - acc: 0.335 - ETA: 4:15 - loss: 2.8100 - acc: 0.335 - ETA: 4:05 - loss: 2.8104 - acc: 0.335 - ETA: 3:55 - loss: 2.8099 - acc: 0.335 - ETA: 3:45 - loss: 2.8095 - acc: 0.335 - ETA: 3:36 - loss: 2.8095 - acc: 0.335 - ETA: 3:26 - loss: 2.8103 - acc: 0.335 - ETA: 3:16 - loss: 2.8102 - acc: 0.335 - ETA: 3:07 - loss: 2.8099 - acc: 0.335 - ETA: 2:57 - loss: 2.8093 - acc: 0.335 - ETA: 2:47 - loss: 2.8090 - acc: 0.335 - ETA: 2:38 - loss: 2.8087 - acc: 0.335 - ETA: 2:28 - loss: 2.8087 - acc: 0.335 - ETA: 2:19 - loss: 2.8084 - acc: 0.335 - ETA: 2:09 - loss: 2.8082 - acc: 0.336 - ETA: 2:00 - loss: 2.8084 - acc: 0.336 - ETA: 1:50 - loss: 2.8082 - acc: 0.336 - ETA: 1:41 - loss: 2.8082 - acc: 0.336 - ETA: 1:31 - loss: 2.8080 - acc: 0.336 - ETA: 1:22 - loss: 2.8074 - acc: 0.336 - ETA: 1:12 - loss: 2.8074 - acc: 0.336 - ETA: 1:03 - loss: 2.8075 - acc: 0.336 - ETA: 54s - loss: 2.8070 - acc: 0.336 - ETA: 44s - loss: 2.8065 - acc: 0.33 - ETA: 35s - loss: 2.8074 - acc: 0.33 - ETA: 25s - loss: 2.8076 - acc: 0.33 - ETA: 16s - loss: 2.8078 - acc: 0.33 - ETA: 7s - loss: 2.8075 - acc: 0.3364 - 1390s 10ms/step - loss: 2.8069 - acc: 0.3366 - val_loss: 2.8105 - val_acc: 0.3402\n",
      "Epoch 6/10\n",
      "133764/133764 [==============================] - ETA: 24:10 - loss: 2.8130 - acc: 0.33 - ETA: 24:10 - loss: 2.7733 - acc: 0.34 - ETA: 24:16 - loss: 2.7786 - acc: 0.34 - ETA: 24:49 - loss: 2.7731 - acc: 0.34 - ETA: 24:02 - loss: 2.7659 - acc: 0.34 - ETA: 23:07 - loss: 2.7639 - acc: 0.34 - ETA: 23:08 - loss: 2.7708 - acc: 0.34 - ETA: 22:53 - loss: 2.7661 - acc: 0.34 - ETA: 22:06 - loss: 2.7658 - acc: 0.34 - ETA: 21:46 - loss: 2.7652 - acc: 0.34 - ETA: 21:17 - loss: 2.7683 - acc: 0.34 - ETA: 21:06 - loss: 2.7655 - acc: 0.34 - ETA: 21:07 - loss: 2.7639 - acc: 0.34 - ETA: 20:55 - loss: 2.7629 - acc: 0.34 - ETA: 20:47 - loss: 2.7628 - acc: 0.34 - ETA: 20:38 - loss: 2.7580 - acc: 0.35 - ETA: 20:24 - loss: 2.7571 - acc: 0.35 - ETA: 20:25 - loss: 2.7576 - acc: 0.35 - ETA: 20:09 - loss: 2.7585 - acc: 0.35 - ETA: 20:01 - loss: 2.7586 - acc: 0.35 - ETA: 19:40 - loss: 2.7612 - acc: 0.35 - ETA: 19:27 - loss: 2.7617 - acc: 0.35 - ETA: 19:14 - loss: 2.7603 - acc: 0.35 - ETA: 19:00 - loss: 2.7590 - acc: 0.35 - ETA: 18:42 - loss: 2.7586 - acc: 0.35 - ETA: 18:30 - loss: 2.7580 - acc: 0.35 - ETA: 18:20 - loss: 2.7562 - acc: 0.35 - ETA: 18:08 - loss: 2.7550 - acc: 0.35 - ETA: 17:56 - loss: 2.7548 - acc: 0.35 - ETA: 17:45 - loss: 2.7540 - acc: 0.35 - ETA: 17:36 - loss: 2.7517 - acc: 0.35 - ETA: 17:35 - loss: 2.7547 - acc: 0.35 - ETA: 17:20 - loss: 2.7544 - acc: 0.35 - ETA: 17:10 - loss: 2.7529 - acc: 0.35 - ETA: 17:10 - loss: 2.7544 - acc: 0.35 - ETA: 16:55 - loss: 2.7545 - acc: 0.35 - ETA: 16:43 - loss: 2.7534 - acc: 0.35 - ETA: 16:26 - loss: 2.7518 - acc: 0.35 - ETA: 16:13 - loss: 2.7516 - acc: 0.35 - ETA: 16:00 - loss: 2.7513 - acc: 0.35 - ETA: 15:46 - loss: 2.7521 - acc: 0.35 - ETA: 15:34 - loss: 2.7511 - acc: 0.35 - ETA: 15:31 - loss: 2.7500 - acc: 0.35 - ETA: 15:22 - loss: 2.7500 - acc: 0.35 - ETA: 15:08 - loss: 2.7500 - acc: 0.35 - ETA: 14:56 - loss: 2.7497 - acc: 0.35 - ETA: 14:46 - loss: 2.7507 - acc: 0.35 - ETA: 14:35 - loss: 2.7524 - acc: 0.35 - ETA: 14:24 - loss: 2.7531 - acc: 0.35 - ETA: 14:14 - loss: 2.7534 - acc: 0.35 - ETA: 14:00 - loss: 2.7536 - acc: 0.35 - ETA: 13:49 - loss: 2.7535 - acc: 0.35 - ETA: 13:38 - loss: 2.7532 - acc: 0.35 - ETA: 13:27 - loss: 2.7539 - acc: 0.35 - ETA: 13:16 - loss: 2.7532 - acc: 0.35 - ETA: 13:04 - loss: 2.7526 - acc: 0.35 - ETA: 12:54 - loss: 2.7531 - acc: 0.35 - ETA: 12:44 - loss: 2.7550 - acc: 0.35 - ETA: 12:35 - loss: 2.7546 - acc: 0.35 - ETA: 12:25 - loss: 2.7567 - acc: 0.35 - ETA: 12:15 - loss: 2.7573 - acc: 0.35 - ETA: 12:07 - loss: 2.7576 - acc: 0.35 - ETA: 11:57 - loss: 2.7583 - acc: 0.35 - ETA: 11:46 - loss: 2.7592 - acc: 0.35 - ETA: 11:35 - loss: 2.7591 - acc: 0.35 - ETA: 11:23 - loss: 2.7591 - acc: 0.35 - ETA: 11:15 - loss: 2.7603 - acc: 0.35 - ETA: 11:03 - loss: 2.7598 - acc: 0.35 - ETA: 10:53 - loss: 2.7602 - acc: 0.35 - ETA: 10:43 - loss: 2.7608 - acc: 0.35 - ETA: 10:32 - loss: 2.7609 - acc: 0.35 - ETA: 10:24 - loss: 2.7603 - acc: 0.35 - ETA: 10:16 - loss: 2.7601 - acc: 0.35 - ETA: 10:04 - loss: 2.7590 - acc: 0.35 - ETA: 9:53 - loss: 2.7596 - acc: 0.3526 - ETA: 9:42 - loss: 2.7602 - acc: 0.352 - ETA: 9:33 - loss: 2.7597 - acc: 0.352 - ETA: 9:23 - loss: 2.7592 - acc: 0.352 - ETA: 9:13 - loss: 2.7594 - acc: 0.352 - ETA: 9:05 - loss: 2.7586 - acc: 0.352 - ETA: 8:57 - loss: 2.7592 - acc: 0.352 - ETA: 8:46 - loss: 2.7589 - acc: 0.352 - ETA: 8:35 - loss: 2.7591 - acc: 0.352 - ETA: 8:24 - loss: 2.7587 - acc: 0.352 - ETA: 8:15 - loss: 2.7591 - acc: 0.352 - ETA: 8:05 - loss: 2.7590 - acc: 0.352 - ETA: 7:54 - loss: 2.7588 - acc: 0.352 - ETA: 7:45 - loss: 2.7581 - acc: 0.353 - ETA: 7:34 - loss: 2.7582 - acc: 0.353 - ETA: 7:23 - loss: 2.7579 - acc: 0.353 - ETA: 7:13 - loss: 2.7573 - acc: 0.353 - ETA: 7:03 - loss: 2.7572 - acc: 0.353 - ETA: 6:52 - loss: 2.7568 - acc: 0.353 - ETA: 6:42 - loss: 2.7562 - acc: 0.353 - ETA: 6:32 - loss: 2.7562 - acc: 0.353 - ETA: 6:22 - loss: 2.7556 - acc: 0.353 - ETA: 6:11 - loss: 2.7564 - acc: 0.353 - ETA: 6:01 - loss: 2.7559 - acc: 0.353 - ETA: 5:51 - loss: 2.7554 - acc: 0.353 - ETA: 5:41 - loss: 2.7556 - acc: 0.353 - ETA: 5:30 - loss: 2.7550 - acc: 0.353 - ETA: 5:20 - loss: 2.7554 - acc: 0.353 - ETA: 5:10 - loss: 2.7551 - acc: 0.353 - ETA: 5:00 - loss: 2.7547 - acc: 0.354 - ETA: 4:49 - loss: 2.7539 - acc: 0.354 - ETA: 4:39 - loss: 2.7539 - acc: 0.354 - ETA: 4:29 - loss: 2.7540 - acc: 0.354 - ETA: 4:19 - loss: 2.7538 - acc: 0.354 - ETA: 4:09 - loss: 2.7539 - acc: 0.354 - ETA: 3:58 - loss: 2.7540 - acc: 0.354 - ETA: 3:48 - loss: 2.7536 - acc: 0.354 - ETA: 3:38 - loss: 2.7537 - acc: 0.354 - ETA: 3:28 - loss: 2.7539 - acc: 0.354 - ETA: 3:18 - loss: 2.7541 - acc: 0.354 - ETA: 3:08 - loss: 2.7543 - acc: 0.354 - ETA: 2:58 - loss: 2.7543 - acc: 0.354 - ETA: 2:48 - loss: 2.7551 - acc: 0.354 - ETA: 2:38 - loss: 2.7556 - acc: 0.354 - ETA: 2:28 - loss: 2.7555 - acc: 0.354 - ETA: 2:18 - loss: 2.7542 - acc: 0.354 - ETA: 2:08 - loss: 2.7533 - acc: 0.354 - ETA: 1:58 - loss: 2.7528 - acc: 0.354 - ETA: 1:48 - loss: 2.7528 - acc: 0.354 - ETA: 1:37 - loss: 2.7532 - acc: 0.354 - ETA: 1:27 - loss: 2.7535 - acc: 0.354 - ETA: 1:17 - loss: 2.7532 - acc: 0.354 - ETA: 1:07 - loss: 2.7534 - acc: 0.354 - ETA: 57s - loss: 2.7532 - acc: 0.354 - ETA: 47s - loss: 2.7533 - acc: 0.35 - ETA: 37s - loss: 2.7534 - acc: 0.35 - ETA: 27s - loss: 2.7529 - acc: 0.35 - ETA: 17s - loss: 2.7529 - acc: 0.35 - ETA: 7s - loss: 2.7530 - acc: 0.3550 - 1440s 11ms/step - loss: 2.7531 - acc: 0.3550 - val_loss: 2.7803 - val_acc: 0.3519\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133764/133764 [==============================] - ETA: 19:20 - loss: 2.7219 - acc: 0.36 - ETA: 18:59 - loss: 2.7381 - acc: 0.36 - ETA: 17:55 - loss: 2.7325 - acc: 0.37 - ETA: 17:54 - loss: 2.7328 - acc: 0.36 - ETA: 17:51 - loss: 2.7331 - acc: 0.36 - ETA: 17:45 - loss: 2.7196 - acc: 0.36 - ETA: 17:19 - loss: 2.7174 - acc: 0.36 - ETA: 17:18 - loss: 2.7224 - acc: 0.36 - ETA: 17:14 - loss: 2.7190 - acc: 0.36 - ETA: 16:52 - loss: 2.7235 - acc: 0.36 - ETA: 16:47 - loss: 2.7213 - acc: 0.36 - ETA: 16:42 - loss: 2.7218 - acc: 0.36 - ETA: 16:24 - loss: 2.7222 - acc: 0.36 - ETA: 16:19 - loss: 2.7227 - acc: 0.36 - ETA: 16:13 - loss: 2.7179 - acc: 0.36 - ETA: 15:58 - loss: 2.7167 - acc: 0.36 - ETA: 15:51 - loss: 2.7142 - acc: 0.36 - ETA: 15:46 - loss: 2.7137 - acc: 0.36 - ETA: 15:40 - loss: 2.7132 - acc: 0.36 - ETA: 15:36 - loss: 2.7127 - acc: 0.36 - ETA: 15:22 - loss: 2.7102 - acc: 0.36 - ETA: 15:15 - loss: 2.7110 - acc: 0.36 - ETA: 15:09 - loss: 2.7123 - acc: 0.36 - ETA: 15:02 - loss: 2.7129 - acc: 0.36 - ETA: 14:56 - loss: 2.7147 - acc: 0.36 - ETA: 14:51 - loss: 2.7137 - acc: 0.36 - ETA: 14:48 - loss: 2.7119 - acc: 0.36 - ETA: 14:40 - loss: 2.7120 - acc: 0.36 - ETA: 14:42 - loss: 2.7115 - acc: 0.36 - ETA: 14:30 - loss: 2.7116 - acc: 0.36 - ETA: 14:23 - loss: 2.7100 - acc: 0.36 - ETA: 14:16 - loss: 2.7111 - acc: 0.36 - ETA: 14:10 - loss: 2.7121 - acc: 0.36 - ETA: 14:00 - loss: 2.7119 - acc: 0.36 - ETA: 13:53 - loss: 2.7134 - acc: 0.36 - ETA: 13:53 - loss: 2.7120 - acc: 0.36 - ETA: 13:45 - loss: 2.7132 - acc: 0.36 - ETA: 13:38 - loss: 2.7157 - acc: 0.36 - ETA: 13:29 - loss: 2.7146 - acc: 0.36 - ETA: 13:24 - loss: 2.7131 - acc: 0.36 - ETA: 13:22 - loss: 2.7127 - acc: 0.36 - ETA: 13:12 - loss: 2.7121 - acc: 0.36 - ETA: 13:05 - loss: 2.7115 - acc: 0.36 - ETA: 13:02 - loss: 2.7115 - acc: 0.36 - ETA: 12:52 - loss: 2.7106 - acc: 0.36 - ETA: 12:46 - loss: 2.7100 - acc: 0.36 - ETA: 12:41 - loss: 2.7100 - acc: 0.36 - ETA: 12:34 - loss: 2.7098 - acc: 0.36 - ETA: 12:24 - loss: 2.7096 - acc: 0.36 - ETA: 12:15 - loss: 2.7077 - acc: 0.36 - ETA: 12:07 - loss: 2.7088 - acc: 0.36 - ETA: 11:59 - loss: 2.7092 - acc: 0.36 - ETA: 11:50 - loss: 2.7092 - acc: 0.36 - ETA: 11:42 - loss: 2.7098 - acc: 0.36 - ETA: 11:33 - loss: 2.7099 - acc: 0.36 - ETA: 11:25 - loss: 2.7114 - acc: 0.36 - ETA: 11:17 - loss: 2.7131 - acc: 0.36 - ETA: 11:07 - loss: 2.7120 - acc: 0.36 - ETA: 10:59 - loss: 2.7125 - acc: 0.36 - ETA: 10:51 - loss: 2.7129 - acc: 0.36 - ETA: 10:43 - loss: 2.7126 - acc: 0.36 - ETA: 10:33 - loss: 2.7115 - acc: 0.36 - ETA: 10:25 - loss: 2.7117 - acc: 0.36 - ETA: 10:17 - loss: 2.7124 - acc: 0.36 - ETA: 10:10 - loss: 2.7121 - acc: 0.36 - ETA: 10:02 - loss: 2.7126 - acc: 0.36 - ETA: 9:52 - loss: 2.7122 - acc: 0.3668 - ETA: 9:45 - loss: 2.7115 - acc: 0.366 - ETA: 9:40 - loss: 2.7113 - acc: 0.366 - ETA: 9:32 - loss: 2.7116 - acc: 0.366 - ETA: 9:22 - loss: 2.7118 - acc: 0.366 - ETA: 9:13 - loss: 2.7123 - acc: 0.366 - ETA: 9:05 - loss: 2.7116 - acc: 0.366 - ETA: 8:58 - loss: 2.7115 - acc: 0.367 - ETA: 8:50 - loss: 2.7109 - acc: 0.367 - ETA: 8:42 - loss: 2.7107 - acc: 0.367 - ETA: 8:33 - loss: 2.7110 - acc: 0.367 - ETA: 8:26 - loss: 2.7112 - acc: 0.367 - ETA: 8:17 - loss: 2.7105 - acc: 0.367 - ETA: 8:09 - loss: 2.7110 - acc: 0.367 - ETA: 8:00 - loss: 2.7112 - acc: 0.367 - ETA: 7:51 - loss: 2.7108 - acc: 0.367 - ETA: 7:43 - loss: 2.7102 - acc: 0.367 - ETA: 7:34 - loss: 2.7104 - acc: 0.367 - ETA: 7:26 - loss: 2.7104 - acc: 0.367 - ETA: 7:17 - loss: 2.7107 - acc: 0.366 - ETA: 7:09 - loss: 2.7114 - acc: 0.366 - ETA: 7:00 - loss: 2.7111 - acc: 0.367 - ETA: 6:51 - loss: 2.7114 - acc: 0.366 - ETA: 6:43 - loss: 2.7119 - acc: 0.366 - ETA: 6:34 - loss: 2.7116 - acc: 0.367 - ETA: 6:25 - loss: 2.7112 - acc: 0.367 - ETA: 6:16 - loss: 2.7116 - acc: 0.367 - ETA: 6:07 - loss: 2.7117 - acc: 0.366 - ETA: 5:58 - loss: 2.7115 - acc: 0.366 - ETA: 5:49 - loss: 2.7110 - acc: 0.366 - ETA: 5:40 - loss: 2.7105 - acc: 0.367 - ETA: 5:31 - loss: 2.7110 - acc: 0.367 - ETA: 5:22 - loss: 2.7108 - acc: 0.367 - ETA: 5:13 - loss: 2.7105 - acc: 0.367 - ETA: 5:04 - loss: 2.7101 - acc: 0.367 - ETA: 4:55 - loss: 2.7104 - acc: 0.367 - ETA: 4:46 - loss: 2.7104 - acc: 0.367 - ETA: 4:37 - loss: 2.7110 - acc: 0.367 - ETA: 4:28 - loss: 2.7107 - acc: 0.367 - ETA: 4:18 - loss: 2.7104 - acc: 0.367 - ETA: 4:09 - loss: 2.7101 - acc: 0.367 - ETA: 4:00 - loss: 2.7109 - acc: 0.367 - ETA: 3:51 - loss: 2.7116 - acc: 0.367 - ETA: 3:42 - loss: 2.7111 - acc: 0.367 - ETA: 3:33 - loss: 2.7117 - acc: 0.366 - ETA: 3:23 - loss: 2.7111 - acc: 0.367 - ETA: 3:14 - loss: 2.7111 - acc: 0.367 - ETA: 3:05 - loss: 2.7112 - acc: 0.367 - ETA: 2:56 - loss: 2.7107 - acc: 0.367 - ETA: 2:46 - loss: 2.7106 - acc: 0.367 - ETA: 2:37 - loss: 2.7101 - acc: 0.367 - ETA: 2:28 - loss: 2.7101 - acc: 0.367 - ETA: 2:19 - loss: 2.7104 - acc: 0.367 - ETA: 2:09 - loss: 2.7108 - acc: 0.367 - ETA: 2:00 - loss: 2.7112 - acc: 0.367 - ETA: 1:51 - loss: 2.7117 - acc: 0.367 - ETA: 1:41 - loss: 2.7110 - acc: 0.367 - ETA: 1:32 - loss: 2.7111 - acc: 0.367 - ETA: 1:22 - loss: 2.7108 - acc: 0.367 - ETA: 1:13 - loss: 2.7104 - acc: 0.367 - ETA: 1:04 - loss: 2.7106 - acc: 0.367 - ETA: 54s - loss: 2.7103 - acc: 0.367 - ETA: 45s - loss: 2.7101 - acc: 0.36 - ETA: 35s - loss: 2.7101 - acc: 0.36 - ETA: 26s - loss: 2.7103 - acc: 0.36 - ETA: 16s - loss: 2.7101 - acc: 0.36 - ETA: 7s - loss: 2.7101 - acc: 0.3680 - 1406s 11ms/step - loss: 2.7099 - acc: 0.3681 - val_loss: 2.7487 - val_acc: 0.3614\n",
      "Epoch 8/10\n",
      "133764/133764 [==============================] - ETA: 25:51 - loss: 2.6840 - acc: 0.38 - ETA: 24:01 - loss: 2.6991 - acc: 0.37 - ETA: 22:23 - loss: 2.6898 - acc: 0.37 - ETA: 22:10 - loss: 2.6768 - acc: 0.37 - ETA: 22:02 - loss: 2.6860 - acc: 0.37 - ETA: 21:53 - loss: 2.6881 - acc: 0.37 - ETA: 21:53 - loss: 2.6852 - acc: 0.37 - ETA: 21:25 - loss: 2.6933 - acc: 0.36 - ETA: 21:38 - loss: 2.6862 - acc: 0.36 - ETA: 21:08 - loss: 2.6745 - acc: 0.37 - ETA: 20:56 - loss: 2.6702 - acc: 0.37 - ETA: 21:00 - loss: 2.6707 - acc: 0.37 - ETA: 20:36 - loss: 2.6776 - acc: 0.37 - ETA: 20:26 - loss: 2.6802 - acc: 0.37 - ETA: 20:39 - loss: 2.6798 - acc: 0.37 - ETA: 20:56 - loss: 2.6770 - acc: 0.37 - ETA: 20:46 - loss: 2.6792 - acc: 0.37 - ETA: 20:22 - loss: 2.6781 - acc: 0.37 - ETA: 20:18 - loss: 2.6785 - acc: 0.37 - ETA: 19:58 - loss: 2.6730 - acc: 0.37 - ETA: 19:53 - loss: 2.6724 - acc: 0.37 - ETA: 19:33 - loss: 2.6729 - acc: 0.37 - ETA: 19:44 - loss: 2.6723 - acc: 0.37 - ETA: 19:46 - loss: 2.6728 - acc: 0.37 - ETA: 19:22 - loss: 2.6725 - acc: 0.37 - ETA: 19:00 - loss: 2.6724 - acc: 0.37 - ETA: 18:38 - loss: 2.6708 - acc: 0.37 - ETA: 18:27 - loss: 2.6696 - acc: 0.37 - ETA: 18:13 - loss: 2.6704 - acc: 0.37 - ETA: 18:15 - loss: 2.6714 - acc: 0.37 - ETA: 18:10 - loss: 2.6708 - acc: 0.37 - ETA: 18:05 - loss: 2.6709 - acc: 0.37 - ETA: 17:51 - loss: 2.6705 - acc: 0.37 - ETA: 17:38 - loss: 2.6703 - acc: 0.37 - ETA: 17:22 - loss: 2.6685 - acc: 0.37 - ETA: 17:16 - loss: 2.6673 - acc: 0.37 - ETA: 17:09 - loss: 2.6673 - acc: 0.37 - ETA: 17:02 - loss: 2.6684 - acc: 0.37 - ETA: 16:46 - loss: 2.6672 - acc: 0.37 - ETA: 16:39 - loss: 2.6676 - acc: 0.37 - ETA: 16:29 - loss: 2.6667 - acc: 0.37 - ETA: 16:12 - loss: 2.6674 - acc: 0.37 - ETA: 15:59 - loss: 2.6672 - acc: 0.37 - ETA: 15:47 - loss: 2.6690 - acc: 0.37 - ETA: 15:37 - loss: 2.6691 - acc: 0.37 - ETA: 15:24 - loss: 2.6689 - acc: 0.37 - ETA: 15:09 - loss: 2.6690 - acc: 0.37 - ETA: 14:57 - loss: 2.6674 - acc: 0.37 - ETA: 14:44 - loss: 2.6687 - acc: 0.37 - ETA: 14:34 - loss: 2.6675 - acc: 0.37 - ETA: 14:21 - loss: 2.6678 - acc: 0.37 - ETA: 14:09 - loss: 2.6680 - acc: 0.37 - ETA: 13:57 - loss: 2.6673 - acc: 0.37 - ETA: 13:48 - loss: 2.6679 - acc: 0.37 - ETA: 13:40 - loss: 2.6657 - acc: 0.37 - ETA: 13:33 - loss: 2.6651 - acc: 0.37 - ETA: 13:25 - loss: 2.6645 - acc: 0.37 - ETA: 13:19 - loss: 2.6658 - acc: 0.37 - ETA: 13:10 - loss: 2.6653 - acc: 0.37 - ETA: 13:01 - loss: 2.6661 - acc: 0.37 - ETA: 12:48 - loss: 2.6669 - acc: 0.37 - ETA: 12:35 - loss: 2.6673 - acc: 0.37 - ETA: 12:26 - loss: 2.6678 - acc: 0.37 - ETA: 12:17 - loss: 2.6684 - acc: 0.37 - ETA: 12:07 - loss: 2.6683 - acc: 0.37 - ETA: 11:54 - loss: 2.6693 - acc: 0.37 - ETA: 11:42 - loss: 2.6705 - acc: 0.37 - ETA: 11:31 - loss: 2.6705 - acc: 0.37 - ETA: 11:19 - loss: 2.6700 - acc: 0.37 - ETA: 11:07 - loss: 2.6700 - acc: 0.37 - ETA: 10:55 - loss: 2.6701 - acc: 0.37 - ETA: 10:44 - loss: 2.6699 - acc: 0.37 - ETA: 10:32 - loss: 2.6705 - acc: 0.37 - ETA: 10:21 - loss: 2.6697 - acc: 0.37 - ETA: 10:10 - loss: 2.6695 - acc: 0.37 - ETA: 10:03 - loss: 2.6699 - acc: 0.37 - ETA: 9:51 - loss: 2.6702 - acc: 0.3796 - ETA: 9:40 - loss: 2.6706 - acc: 0.379 - ETA: 9:28 - loss: 2.6711 - acc: 0.379 - ETA: 9:17 - loss: 2.6725 - acc: 0.379 - ETA: 9:06 - loss: 2.6720 - acc: 0.379 - ETA: 8:55 - loss: 2.6725 - acc: 0.379 - ETA: 8:44 - loss: 2.6728 - acc: 0.379 - ETA: 8:34 - loss: 2.6716 - acc: 0.379 - ETA: 8:23 - loss: 2.6714 - acc: 0.379 - ETA: 8:12 - loss: 2.6716 - acc: 0.379 - ETA: 8:01 - loss: 2.6719 - acc: 0.379 - ETA: 7:50 - loss: 2.6716 - acc: 0.379 - ETA: 7:39 - loss: 2.6710 - acc: 0.379 - ETA: 7:28 - loss: 2.6715 - acc: 0.379 - ETA: 7:17 - loss: 2.6718 - acc: 0.379 - ETA: 7:06 - loss: 2.6721 - acc: 0.379 - ETA: 6:56 - loss: 2.6727 - acc: 0.379 - ETA: 6:46 - loss: 2.6724 - acc: 0.379 - ETA: 6:35 - loss: 2.6718 - acc: 0.379 - ETA: 6:25 - loss: 2.6709 - acc: 0.379 - ETA: 6:14 - loss: 2.6705 - acc: 0.379 - ETA: 6:04 - loss: 2.6719 - acc: 0.379 - ETA: 5:54 - loss: 2.6719 - acc: 0.379 - ETA: 5:44 - loss: 2.6708 - acc: 0.379 - ETA: 5:34 - loss: 2.6703 - acc: 0.379 - ETA: 5:23 - loss: 2.6702 - acc: 0.379 - ETA: 5:13 - loss: 2.6704 - acc: 0.379 - ETA: 5:03 - loss: 2.6701 - acc: 0.379 - ETA: 4:53 - loss: 2.6708 - acc: 0.379 - ETA: 4:43 - loss: 2.6706 - acc: 0.379 - ETA: 4:32 - loss: 2.6706 - acc: 0.379 - ETA: 4:22 - loss: 2.6702 - acc: 0.379 - ETA: 4:11 - loss: 2.6698 - acc: 0.379 - ETA: 4:01 - loss: 2.6703 - acc: 0.379 - ETA: 3:50 - loss: 2.6696 - acc: 0.379 - ETA: 3:40 - loss: 2.6699 - acc: 0.380 - ETA: 3:30 - loss: 2.6700 - acc: 0.379 - ETA: 3:19 - loss: 2.6698 - acc: 0.380 - ETA: 3:09 - loss: 2.6694 - acc: 0.380 - ETA: 2:59 - loss: 2.6687 - acc: 0.380 - ETA: 2:48 - loss: 2.6682 - acc: 0.380 - ETA: 2:38 - loss: 2.6677 - acc: 0.380 - ETA: 2:28 - loss: 2.6670 - acc: 0.380 - ETA: 2:18 - loss: 2.6669 - acc: 0.380 - ETA: 2:08 - loss: 2.6667 - acc: 0.380 - ETA: 1:58 - loss: 2.6668 - acc: 0.380 - ETA: 1:47 - loss: 2.6666 - acc: 0.381 - ETA: 1:37 - loss: 2.6670 - acc: 0.380 - ETA: 1:27 - loss: 2.6673 - acc: 0.380 - ETA: 1:17 - loss: 2.6672 - acc: 0.380 - ETA: 1:07 - loss: 2.6663 - acc: 0.381 - ETA: 57s - loss: 2.6664 - acc: 0.381 - ETA: 47s - loss: 2.6662 - acc: 0.38 - ETA: 37s - loss: 2.6664 - acc: 0.38 - ETA: 27s - loss: 2.6660 - acc: 0.38 - ETA: 17s - loss: 2.6656 - acc: 0.38 - ETA: 7s - loss: 2.6659 - acc: 0.3813 - 1453s 11ms/step - loss: 2.6665 - acc: 0.3810 - val_loss: 2.7232 - val_acc: 0.3691\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133764/133764 [==============================] - ETA: 20:03 - loss: 2.5818 - acc: 0.38 - ETA: 19:37 - loss: 2.6017 - acc: 0.39 - ETA: 18:22 - loss: 2.6323 - acc: 0.38 - ETA: 18:39 - loss: 2.6261 - acc: 0.38 - ETA: 18:21 - loss: 2.6319 - acc: 0.38 - ETA: 18:31 - loss: 2.6415 - acc: 0.38 - ETA: 38:57 - loss: 2.6366 - acc: 0.38 - ETA: 36:28 - loss: 2.6410 - acc: 0.38 - ETA: 34:30 - loss: 2.6348 - acc: 0.38 - ETA: 32:25 - loss: 2.6349 - acc: 0.38 - ETA: 30:57 - loss: 2.6338 - acc: 0.38 - ETA: 29:40 - loss: 2.6365 - acc: 0.38 - ETA: 28:38 - loss: 2.6344 - acc: 0.38 - ETA: 27:37 - loss: 2.6333 - acc: 0.38 - ETA: 26:45 - loss: 2.6321 - acc: 0.38 - ETA: 25:46 - loss: 2.6321 - acc: 0.38 - ETA: 25:00 - loss: 2.6351 - acc: 0.38 - ETA: 24:24 - loss: 2.6365 - acc: 0.38 - ETA: 23:47 - loss: 2.6377 - acc: 0.38 - ETA: 23:17 - loss: 2.6410 - acc: 0.38 - ETA: 22:47 - loss: 2.6404 - acc: 0.38 - ETA: 22:14 - loss: 2.6362 - acc: 0.38 - ETA: 21:44 - loss: 2.6348 - acc: 0.38 - ETA: 21:19 - loss: 2.6354 - acc: 0.38 - ETA: 20:54 - loss: 2.6364 - acc: 0.38 - ETA: 20:32 - loss: 2.6345 - acc: 0.38 - ETA: 20:09 - loss: 2.6327 - acc: 0.38 - ETA: 19:51 - loss: 2.6334 - acc: 0.38 - ETA: 19:30 - loss: 2.6324 - acc: 0.38 - ETA: 19:24 - loss: 2.6321 - acc: 0.38 - ETA: 19:02 - loss: 2.6288 - acc: 0.38 - ETA: 18:47 - loss: 2.6290 - acc: 0.38 - ETA: 18:29 - loss: 2.6280 - acc: 0.38 - ETA: 18:10 - loss: 2.6276 - acc: 0.38 - ETA: 17:49 - loss: 2.6266 - acc: 0.38 - ETA: 17:31 - loss: 2.6269 - acc: 0.38 - ETA: 17:15 - loss: 2.6287 - acc: 0.38 - ETA: 16:58 - loss: 2.6291 - acc: 0.38 - ETA: 16:41 - loss: 2.6299 - acc: 0.38 - ETA: 16:23 - loss: 2.6296 - acc: 0.38 - ETA: 16:08 - loss: 2.6308 - acc: 0.38 - ETA: 15:53 - loss: 2.6304 - acc: 0.38 - ETA: 15:38 - loss: 2.6293 - acc: 0.38 - ETA: 15:21 - loss: 2.6289 - acc: 0.38 - ETA: 15:07 - loss: 2.6275 - acc: 0.38 - ETA: 14:55 - loss: 2.6275 - acc: 0.38 - ETA: 14:42 - loss: 2.6279 - acc: 0.38 - ETA: 14:29 - loss: 2.6274 - acc: 0.38 - ETA: 14:15 - loss: 2.6268 - acc: 0.38 - ETA: 14:00 - loss: 2.6252 - acc: 0.39 - ETA: 13:47 - loss: 2.6251 - acc: 0.39 - ETA: 13:35 - loss: 2.6264 - acc: 0.39 - ETA: 13:22 - loss: 2.6253 - acc: 0.39 - ETA: 13:10 - loss: 2.6271 - acc: 0.39 - ETA: 12:56 - loss: 2.6270 - acc: 0.39 - ETA: 12:44 - loss: 2.6264 - acc: 0.39 - ETA: 12:32 - loss: 2.6260 - acc: 0.39 - ETA: 12:21 - loss: 2.6258 - acc: 0.39 - ETA: 12:09 - loss: 2.6258 - acc: 0.39 - ETA: 11:57 - loss: 2.6255 - acc: 0.39 - ETA: 11:46 - loss: 2.6256 - acc: 0.39 - ETA: 11:34 - loss: 2.6254 - acc: 0.39 - ETA: 11:23 - loss: 2.6250 - acc: 0.39 - ETA: 11:11 - loss: 2.6246 - acc: 0.39 - ETA: 11:00 - loss: 2.6256 - acc: 0.39 - ETA: 10:50 - loss: 2.6254 - acc: 0.39 - ETA: 10:39 - loss: 2.6247 - acc: 0.39 - ETA: 10:27 - loss: 2.6245 - acc: 0.39 - ETA: 10:17 - loss: 2.6253 - acc: 0.39 - ETA: 10:07 - loss: 2.6260 - acc: 0.39 - ETA: 9:57 - loss: 2.6255 - acc: 0.3909 - ETA: 9:47 - loss: 2.6260 - acc: 0.390 - ETA: 9:37 - loss: 2.6264 - acc: 0.390 - ETA: 9:28 - loss: 2.6263 - acc: 0.390 - ETA: 9:20 - loss: 2.6257 - acc: 0.390 - ETA: 9:08 - loss: 2.6257 - acc: 0.390 - ETA: 8:58 - loss: 2.6249 - acc: 0.390 - ETA: 8:49 - loss: 2.6262 - acc: 0.390 - ETA: 8:39 - loss: 2.6263 - acc: 0.390 - ETA: 8:28 - loss: 2.6260 - acc: 0.390 - ETA: 8:18 - loss: 2.6251 - acc: 0.390 - ETA: 8:07 - loss: 2.6252 - acc: 0.390 - ETA: 7:57 - loss: 2.6245 - acc: 0.390 - ETA: 7:49 - loss: 2.6240 - acc: 0.390 - ETA: 7:40 - loss: 2.6239 - acc: 0.390 - ETA: 7:31 - loss: 2.6252 - acc: 0.390 - ETA: 7:22 - loss: 2.6255 - acc: 0.390 - ETA: 7:12 - loss: 2.6246 - acc: 0.390 - ETA: 7:03 - loss: 2.6253 - acc: 0.390 - ETA: 6:54 - loss: 2.6262 - acc: 0.390 - ETA: 6:44 - loss: 2.6264 - acc: 0.390 - ETA: 6:35 - loss: 2.6264 - acc: 0.389 - ETA: 6:25 - loss: 2.6261 - acc: 0.390 - ETA: 6:16 - loss: 2.6254 - acc: 0.390 - ETA: 6:07 - loss: 2.6252 - acc: 0.390 - ETA: 5:57 - loss: 2.6252 - acc: 0.390 - ETA: 5:47 - loss: 2.6250 - acc: 0.390 - ETA: 5:38 - loss: 2.6256 - acc: 0.390 - ETA: 5:28 - loss: 2.6260 - acc: 0.390 - ETA: 5:19 - loss: 2.6269 - acc: 0.389 - ETA: 5:10 - loss: 2.6271 - acc: 0.389 - ETA: 5:00 - loss: 2.6278 - acc: 0.389 - ETA: 4:51 - loss: 2.6285 - acc: 0.389 - ETA: 4:41 - loss: 2.6278 - acc: 0.389 - ETA: 4:32 - loss: 2.6272 - acc: 0.390 - ETA: 4:22 - loss: 2.6282 - acc: 0.389 - ETA: 4:13 - loss: 2.6287 - acc: 0.389 - ETA: 4:04 - loss: 2.6289 - acc: 0.389 - ETA: 3:54 - loss: 2.6293 - acc: 0.389 - ETA: 3:45 - loss: 2.6288 - acc: 0.389 - ETA: 3:35 - loss: 2.6290 - acc: 0.389 - ETA: 3:26 - loss: 2.6290 - acc: 0.389 - ETA: 3:16 - loss: 2.6290 - acc: 0.389 - ETA: 3:07 - loss: 2.6290 - acc: 0.389 - ETA: 2:58 - loss: 2.6289 - acc: 0.389 - ETA: 2:48 - loss: 2.6293 - acc: 0.389 - ETA: 2:38 - loss: 2.6292 - acc: 0.389 - ETA: 2:29 - loss: 2.6293 - acc: 0.389 - ETA: 2:19 - loss: 2.6292 - acc: 0.389 - ETA: 2:10 - loss: 2.6289 - acc: 0.390 - ETA: 2:01 - loss: 2.6287 - acc: 0.390 - ETA: 1:51 - loss: 2.6287 - acc: 0.390 - ETA: 1:42 - loss: 2.6288 - acc: 0.390 - ETA: 1:32 - loss: 2.6288 - acc: 0.390 - ETA: 1:23 - loss: 2.6281 - acc: 0.390 - ETA: 1:13 - loss: 2.6272 - acc: 0.390 - ETA: 1:04 - loss: 2.6283 - acc: 0.390 - ETA: 54s - loss: 2.6280 - acc: 0.390 - ETA: 45s - loss: 2.6281 - acc: 0.39 - ETA: 35s - loss: 2.6285 - acc: 0.39 - ETA: 26s - loss: 2.6281 - acc: 0.39 - ETA: 16s - loss: 2.6274 - acc: 0.39 - ETA: 7s - loss: 2.6277 - acc: 0.3907 - 1394s 10ms/step - loss: 2.6285 - acc: 0.3906 - val_loss: 2.7047 - val_acc: 0.3745\n",
      "Epoch 10/10\n",
      "133764/133764 [==============================] - ETA: 26:06 - loss: 2.5742 - acc: 0.41 - ETA: 26:52 - loss: 2.5878 - acc: 0.40 - ETA: 25:26 - loss: 2.6013 - acc: 0.40 - ETA: 24:37 - loss: 2.6042 - acc: 0.40 - ETA: 23:54 - loss: 2.6051 - acc: 0.40 - ETA: 23:27 - loss: 2.5987 - acc: 0.40 - ETA: 23:02 - loss: 2.5949 - acc: 0.40 - ETA: 22:45 - loss: 2.5933 - acc: 0.40 - ETA: 22:03 - loss: 2.5968 - acc: 0.40 - ETA: 21:42 - loss: 2.5977 - acc: 0.40 - ETA: 21:19 - loss: 2.5960 - acc: 0.40 - ETA: 20:59 - loss: 2.6043 - acc: 0.39 - ETA: 21:02 - loss: 2.6019 - acc: 0.39 - ETA: 20:51 - loss: 2.6025 - acc: 0.39 - ETA: 20:36 - loss: 2.5997 - acc: 0.39 - ETA: 20:15 - loss: 2.5979 - acc: 0.39 - ETA: 19:44 - loss: 2.5945 - acc: 0.40 - ETA: 19:21 - loss: 2.5980 - acc: 0.40 - ETA: 19:13 - loss: 2.5951 - acc: 0.40 - ETA: 19:00 - loss: 2.5968 - acc: 0.40 - ETA: 18:42 - loss: 2.5960 - acc: 0.40 - ETA: 18:19 - loss: 2.5968 - acc: 0.39 - ETA: 18:03 - loss: 2.5923 - acc: 0.40 - ETA: 17:49 - loss: 2.5920 - acc: 0.40 - ETA: 17:40 - loss: 2.5938 - acc: 0.40 - ETA: 17:28 - loss: 2.5933 - acc: 0.39 - ETA: 17:15 - loss: 2.5928 - acc: 0.40 - ETA: 17:05 - loss: 2.5934 - acc: 0.40 - ETA: 16:57 - loss: 2.5935 - acc: 0.40 - ETA: 16:49 - loss: 2.5944 - acc: 0.39 - ETA: 16:32 - loss: 2.5948 - acc: 0.39 - ETA: 16:22 - loss: 2.5937 - acc: 0.39 - ETA: 16:11 - loss: 2.5946 - acc: 0.39 - ETA: 16:00 - loss: 2.5967 - acc: 0.39 - ETA: 15:50 - loss: 2.5975 - acc: 0.39 - ETA: 15:40 - loss: 2.5983 - acc: 0.39 - ETA: 15:37 - loss: 2.5969 - acc: 0.39 - ETA: 15:30 - loss: 2.5974 - acc: 0.39 - ETA: 15:27 - loss: 2.5980 - acc: 0.39 - ETA: 15:24 - loss: 2.6001 - acc: 0.39 - ETA: 15:22 - loss: 2.5993 - acc: 0.39 - ETA: 15:18 - loss: 2.5990 - acc: 0.39 - ETA: 15:07 - loss: 2.5994 - acc: 0.39 - ETA: 14:57 - loss: 2.5975 - acc: 0.39 - ETA: 14:48 - loss: 2.5986 - acc: 0.39 - ETA: 14:36 - loss: 2.5987 - acc: 0.39 - ETA: 14:26 - loss: 2.5995 - acc: 0.39 - ETA: 14:14 - loss: 2.5994 - acc: 0.39 - ETA: 14:01 - loss: 2.5984 - acc: 0.39 - ETA: 13:47 - loss: 2.5966 - acc: 0.39 - ETA: 13:35 - loss: 2.5959 - acc: 0.39 - ETA: 13:23 - loss: 2.5937 - acc: 0.39 - ETA: 13:11 - loss: 2.5942 - acc: 0.39 - ETA: 13:00 - loss: 2.5950 - acc: 0.39 - ETA: 12:50 - loss: 2.5955 - acc: 0.39 - ETA: 12:39 - loss: 2.5964 - acc: 0.39 - ETA: 12:29 - loss: 2.5960 - acc: 0.39 - ETA: 12:20 - loss: 2.5952 - acc: 0.39 - ETA: 12:11 - loss: 2.5961 - acc: 0.39 - ETA: 12:00 - loss: 2.5960 - acc: 0.39 - ETA: 11:51 - loss: 2.5960 - acc: 0.39 - ETA: 11:43 - loss: 2.5958 - acc: 0.39 - ETA: 11:33 - loss: 2.5949 - acc: 0.39 - ETA: 11:24 - loss: 2.5961 - acc: 0.39 - ETA: 11:14 - loss: 2.5979 - acc: 0.39 - ETA: 11:04 - loss: 2.5978 - acc: 0.39 - ETA: 10:54 - loss: 2.5970 - acc: 0.39 - ETA: 10:46 - loss: 2.5980 - acc: 0.39 - ETA: 10:34 - loss: 2.5985 - acc: 0.39 - ETA: 10:24 - loss: 2.5990 - acc: 0.39 - ETA: 10:12 - loss: 2.5980 - acc: 0.39 - ETA: 10:01 - loss: 2.5975 - acc: 0.39 - ETA: 9:50 - loss: 2.5972 - acc: 0.3986 - ETA: 9:40 - loss: 2.5980 - acc: 0.398 - ETA: 9:29 - loss: 2.5969 - acc: 0.398 - ETA: 9:19 - loss: 2.5973 - acc: 0.398 - ETA: 9:08 - loss: 2.5971 - acc: 0.398 - ETA: 8:58 - loss: 2.5976 - acc: 0.398 - ETA: 8:49 - loss: 2.5974 - acc: 0.398 - ETA: 8:41 - loss: 2.5969 - acc: 0.399 - ETA: 8:32 - loss: 2.5958 - acc: 0.399 - ETA: 8:22 - loss: 2.5965 - acc: 0.399 - ETA: 8:11 - loss: 2.5970 - acc: 0.398 - ETA: 8:02 - loss: 2.5971 - acc: 0.398 - ETA: 7:52 - loss: 2.5968 - acc: 0.398 - ETA: 7:43 - loss: 2.5961 - acc: 0.398 - ETA: 7:33 - loss: 2.5964 - acc: 0.399 - ETA: 7:22 - loss: 2.5962 - acc: 0.399 - ETA: 7:12 - loss: 2.5958 - acc: 0.399 - ETA: 7:02 - loss: 2.5952 - acc: 0.399 - ETA: 6:52 - loss: 2.5953 - acc: 0.399 - ETA: 6:42 - loss: 2.5952 - acc: 0.399 - ETA: 6:32 - loss: 2.5948 - acc: 0.399 - ETA: 6:22 - loss: 2.5943 - acc: 0.399 - ETA: 6:11 - loss: 2.5938 - acc: 0.400 - ETA: 6:01 - loss: 2.5944 - acc: 0.399 - ETA: 5:51 - loss: 2.5946 - acc: 0.399 - ETA: 5:42 - loss: 2.5948 - acc: 0.399 - ETA: 5:32 - loss: 2.5945 - acc: 0.399 - ETA: 5:22 - loss: 2.5942 - acc: 0.400 - ETA: 5:12 - loss: 2.5944 - acc: 0.400 - ETA: 5:02 - loss: 2.5947 - acc: 0.400 - ETA: 4:52 - loss: 2.5940 - acc: 0.400 - ETA: 4:42 - loss: 2.5940 - acc: 0.400 - ETA: 4:33 - loss: 2.5937 - acc: 0.400 - ETA: 4:23 - loss: 2.5933 - acc: 0.400 - ETA: 4:13 - loss: 2.5925 - acc: 0.400 - ETA: 4:03 - loss: 2.5925 - acc: 0.400 - ETA: 3:54 - loss: 2.5926 - acc: 0.401 - ETA: 3:44 - loss: 2.5927 - acc: 0.400 - ETA: 3:34 - loss: 2.5928 - acc: 0.401 - ETA: 3:25 - loss: 2.5927 - acc: 0.401 - ETA: 3:15 - loss: 2.5928 - acc: 0.401 - ETA: 3:05 - loss: 2.5928 - acc: 0.401 - ETA: 2:56 - loss: 2.5931 - acc: 0.401 - ETA: 2:46 - loss: 2.5936 - acc: 0.401 - ETA: 2:37 - loss: 2.5935 - acc: 0.401 - ETA: 2:27 - loss: 2.5935 - acc: 0.401 - ETA: 2:18 - loss: 2.5940 - acc: 0.400 - ETA: 2:08 - loss: 2.5941 - acc: 0.400 - ETA: 1:59 - loss: 2.5937 - acc: 0.400 - ETA: 1:50 - loss: 2.5931 - acc: 0.401 - ETA: 1:40 - loss: 2.5939 - acc: 0.401 - ETA: 1:31 - loss: 2.5938 - acc: 0.401 - ETA: 1:21 - loss: 2.5932 - acc: 0.401 - ETA: 1:12 - loss: 2.5933 - acc: 0.401 - ETA: 1:03 - loss: 2.5938 - acc: 0.401 - ETA: 53s - loss: 2.5926 - acc: 0.401 - ETA: 44s - loss: 2.5915 - acc: 0.40 - ETA: 35s - loss: 2.5914 - acc: 0.40 - ETA: 25s - loss: 2.5912 - acc: 0.40 - ETA: 16s - loss: 2.5907 - acc: 0.40 - ETA: 7s - loss: 2.5911 - acc: 0.4018 - 1369s 10ms/step - loss: 2.5914 - acc: 0.4016 - val_loss: 2.6771 - val_acc: 0.3931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2973271b780>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modele d'apprentissage CNN avec 3 couches de convolutions , et 3 couches de maxpooling\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)\n",
    "from keras.layers import Embedding\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM)\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "sequence_length = X_train.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=41, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs, output)\n",
    "\n",
    "adam = Adam(lr=1e-3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])\n",
    "callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "model.fit(X_train, y_train, batch_size=1000, epochs=10, verbose=1, validation_data=(X_val, y_test),\n",
    "         callbacks=callbacks)  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression after TF IDF\n",
    "vectorizer_char = TfidfVectorizer(max_features=40000,\n",
    "                             min_df=5, \n",
    "                             max_df=0.5, \n",
    "                             analyzer='char', \n",
    "                             ngram_range=(1, 4))\n",
    "\n",
    "vectorizer_char.fit(x_train);\n",
    "\n",
    "tfidf_matrix_char_train = vectorizer_char.transform(x_train)\n",
    "tfidf_matrix_char_test = vectorizer_char.transform(x_test)\n",
    "\n",
    "lr_char = LogisticRegression(solver='sag', verbose=2)\n",
    "lr_char.fit(tfidf_matrix_char_train, y_train)\n",
    "\n",
    "y_pred_char = lr_char.predict(tfidf_matrix_char_test)\n",
    "joblib.dump(lr_char, 'lr_char_ngram.pkl')\n",
    "\n",
    "pd.DataFrame(y_pred_char, columns=['y_pred']).to_csv('lr_char_ngram.csv', index=False)\n",
    "y_pred_char = pd.read_csv('lr_char_ngram.csv')\n",
    "print(accuracy_score(y_test, y_pred_char))\n",
    "tfidf_matrix_word_char_train =  hstack((tfidf_matrix_word_train, tfidf_matrix_char_train))\n",
    "tfidf_matrix_word_char_test =  hstack((tfidf_matrix_word_test, tfidf_matrix_char_test))\n",
    "\n",
    "lr_word_char = LogisticRegression(solver='sag', verbose=2)\n",
    "lr_word_char.fit(tfidf_matrix_word_char_train, y_train)\n",
    "\n",
    "y_pred_word_char = lr_word_char.predict(tfidf_matrix_word_char_test)\n",
    "joblib.dump(lr_word_char, 'lr_word_char_ngram.pkl')\n",
    "\n",
    "pd.DataFrame(y_pred_word_char, columns=['y_pred']).to_csv('lr_word_char_ngram.csv', index=False)\n",
    "y_pred_word_char = pd.read_csv('lr_word_char_ngram.csv')\n",
    "print(accuracy_score(y_test, y_pred_word_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('/Users/shangavi/Desktop/AllNews.csv')\n",
    "cl=TextCleaner(None, None)\n",
    "cl.add_stopwords(stopwords)\n",
    "test[\"clean_text\"]=[cl.clean(w) for w in test[\"description\"]]\n",
    "test[\"tokens\"]=[cl.tokenize(w, remove_stopwords=True, substitute=False,lemmatizer=lemmatizer) for w in test[\"clean_text\"]]\n",
    "test['text']=[\" \".join (token) for token in test['tokens']]\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(test.text)\n",
    "padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "pred = model.predict(padded)\n",
    "labels = data.category\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
